{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ac60cf",
   "metadata": {},
   "source": [
    "# Master Format Converter & Imputation Audit\n",
    "\n",
    "This notebook implements the canonical conversion pipeline to make the full master\n",
    "training dataset match the 10k sample format (uniform `created_at`, imputation\n",
    "of missing timestamps with audit flags). Use it to run the conversion interactively\n",
    "and to validate the produced master file prior to writing the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb02b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Load and Inspect Master Training Sample\n",
    "SAMPLE_PATH = Path('../master_training_data/master_training_sample_10kv3_imputed.csv')\n",
    "if not SAMPLE_PATH.exists():\n",
    "    SAMPLE_PATH = Path('../master_training_data/master_training_sample_10kv3.csv')\n",
    "\n",
    "print('Loading sample from', SAMPLE_PATH)\n",
    "df_sample = pd.read_csv(SAMPLE_PATH, low_memory=False)\n",
    "print('Rows:', len(df_sample))\n",
    "print('Columns:', list(df_sample.columns))\n",
    "print('\\ncreated_at NA:', pd.to_datetime(df_sample['created_at'], errors='coerce').isna().sum())\n",
    "print('\\nSample rows (head 3):')\n",
    "print(df_sample.head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6045f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Define Expected Output Schema\n",
    "EXPECTED_COLUMNS = [\n",
    "    'text',\n",
    "    'emotion_fear','emotion_sadness','emotion_anger','emotion_nervousness','emotion_disgust','emotion_surprise','emotion_confusion','emotion_caring','emotion_grief','emotion_disappointment','emotion_joy','emotion_relief','emotion_neutral',\n",
    "    'event_type','informativeness','crisis_label','source_dataset','created_at','created_at_imputed','created_at_imputed_method'\n",
    "]\n",
    "\n",
    "print('Expected columns count:', len(EXPECTED_COLUMNS))\n",
    "\n",
    "\n",
    "def validate_schema(df):\n",
    "    missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "    extra = [c for c in df.columns if c not in EXPECTED_COLUMNS]\n",
    "    return missing, extra\n",
    "\n",
    "print('Missing/extra in sample:', validate_schema(df_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e170011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Implement Field-wise Transformation Functions\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Simple text normalization: trim and collapse whitespace.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return ''\n",
    "    return ' '.join(str(s).split())\n",
    "\n",
    "\n",
    "def parse_created_at(s: str):\n",
    "    \"\"\"Parse various timestamp formats into a standardized string.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    try:\n",
    "        dt = pd.to_datetime(s, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            return None\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# quick unit tests\n",
    "assert normalize_text('  Hello   world\\n') == 'Hello world'\n",
    "assert parse_created_at('2018-07-01T12:00:00Z') == '2018-07-01 12:00:00'\n",
    "print('Field functions OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f51758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Normalize and Validate Records\n",
    "\n",
    "def convert_record(row):\n",
    "    out = {}\n",
    "    out['text'] = normalize_text(row.get('text'))\n",
    "    for col in EMOTION_COLUMNS:\n",
    "        out[col] = int(row.get(col) or 0)\n",
    "    out['event_type'] = row.get('event_type')\n",
    "    out['informativeness'] = row.get('informativeness')\n",
    "    out['crisis_label'] = row.get('crisis_label')\n",
    "    out['source_dataset'] = row.get('source_dataset')\n",
    "    out['created_at'] = parse_created_at(row.get('created_at'))\n",
    "    out['created_at_imputed'] = bool(row.get('created_at_imputed'))\n",
    "    out['created_at_imputed_method'] = row.get('created_at_imputed_method')\n",
    "    return out\n",
    "\n",
    "# Test conversion on sample\n",
    "test_out = [convert_record(dict(df_sample.iloc[i])) for i in range(3)]\n",
    "print('Converted sample records:')\n",
    "print(test_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b070513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Batch Processing Pipeline for Master Data\n",
    "\n",
    "MASTER_IN = Path('../master_training_data/master_training_data_v3.csv')\n",
    "MASTER_OUT = Path('../master_training_data/master_training_data_v4.csv')\n",
    "\n",
    "print('Master input exists:', MASTER_IN.exists())\n",
    "\n",
    "\n",
    "def process_master_chunk(df_chunk):\n",
    "    records = [convert_record(dict(r)) for _, r in df_chunk.iterrows()]\n",
    "    out_df = pd.DataFrame(records)\n",
    "    # Validate schema\n",
    "    missing, extra = validate_schema(out_df)\n",
    "    return out_df, missing, extra\n",
    "\n",
    "# Example running on a small slice (do not run on full file interactively here unless you opt-in)\n",
    "if MASTER_IN.exists():\n",
    "    sample = pd.read_csv(MASTER_IN, nrows=100)\n",
    "    out_df, missing, extra = process_master_chunk(sample)\n",
    "    print('Processed sample rows:', len(out_df), 'missing cols:', missing, 'extra cols:', extra)\n",
    "else:\n",
    "    print('MASTER_IN not present locally; run in environment with the master file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Compare Produced Format to Sample (Automated Checks)\n",
    "\n",
    "def compare_schema(a: pd.DataFrame, b: pd.DataFrame):\n",
    "    return set(a.columns) == set(b.columns)\n",
    "\n",
    "if 'df_sample' in globals():\n",
    "    print('Sample columns count:', len(df_sample.columns))\n",
    "    # do a small transform check\n",
    "    print('Transform record equality test (first row):')\n",
    "    transformed = convert_record(dict(df_sample.iloc[0]))\n",
    "    print('Transformed keys:', transformed.keys())\n",
    "else:\n",
    "    print('No sample loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670bf197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Save Formatted Master Training Data (idempotent)\n",
    "\n",
    "def save_master(df_out, path=MASTER_OUT):\n",
    "    if path.exists():\n",
    "        backup = path.with_suffix('.bak')\n",
    "        print('Backing up existing master to', backup)\n",
    "        path.rename(backup)\n",
    "    df_out.to_csv(path, index=False)\n",
    "    print('Saved formatted master to', path)\n",
    "\n",
    "# NOTE: Run this only when you are ready to write the full dataset (it may be large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d034c1",
   "metadata": {},
   "source": [
    "# Section 9: CLI/Script Entrypoint (example usage)\n",
    "\n",
    "# In practice you would use the scripts/phase4_combine/create_master_training_file.py\n",
    "# This notebook demonstrates how to call the conversion logic programmatically.\n",
    "\n",
    "print('To run full conversion, call the script with WRITE_FULL_MASTER = True and verify outputs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2be85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Unit Tests and Example Runs\n",
    "\n",
    "# Simple inline test: transform first 10 rows of the sample and ensure schema validity\n",
    "if 'df_sample' in globals():\n",
    "    transformed = pd.DataFrame([convert_record(dict(r)) for _, r in df_sample.head(10).iterrows()])\n",
    "    missing, extra = validate_schema(transformed)\n",
    "    print('Transformed rows:', len(transformed), 'missing cols:', missing, 'extra cols:', extra)\n",
    "else:\n",
    "    print('No sample loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7975a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Logging, Error Handling, and Reporting\n",
    "\n",
    "import json\n",
    "\n",
    "def generate_report(valid_count, error_examples):\n",
    "    report = {\n",
    "        'valid_count': int(valid_count),\n",
    "        'error_examples': error_examples,\n",
    "        'generated_at': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    print(json.dumps(report, indent=2))\n",
    "\n",
    "print('Notebook ready: use cells above to run the conversion interactively and save outputs when ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26024fef",
   "metadata": {},
   "source": [
    "# Imputation audit & master conversion\n",
    "# Purpose: Validate imputed timestamps, compare distributions (hour/day), spot-check rows, and produce final master-format export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "SAMPLE_IMPUTED = Path('master_training_data/master_training_sample_10kv3_imputed.csv')\n",
    "REPORT_DIR = Path('reports/imputation')\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & basic checks\n",
    "df = pd.read_csv(SAMPLE_IMPUTED, parse_dates=['created_at'], na_values=[''])\n",
    "print('rows:', len(df))\n",
    "print('imputed_count:', int(df['created_at_imputed'].sum()))\n",
    "\n",
    "# Save quick CSV with imputed row ids\n",
    "imputed_rows = df.loc[df['created_at_imputed'], ['text']].reset_index().rename(columns={'index': 'row_index'})\n",
    "imputed_rows.to_csv(REPORT_DIR / 'imputed_rows_sample.csv', index=False)\n",
    "print('Saved imputed row index CSV to', REPORT_DIR / 'imputed_rows_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a719ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour-of-day comparison for real vs imputed\n",
    "# Prepare\n",
    "if 'created_at_imputed' not in df.columns:\n",
    "    raise ValueError('No imputed column found - run imputation first')\n",
    "\n",
    "# Extract hour\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "real_hours = df.loc[~df['created_at_imputed'], 'hour']\n",
    "imp_hours = df.loc[df['created_at_imputed'], 'hour']\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist([real_hours, imp_hours], bins=24, label=['real','imputed'], alpha=0.75)\n",
    "plt.legend(); plt.title('Hour-of-day: real vs imputed'); plt.xlabel('hour'); plt.show()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(REPORT_DIR / 'hour_of_day_real_vs_imputed.png', bbox_inches='tight')\n",
    "print('Saved plot to', REPORT_DIR / 'hour_of_day_real_vs_imputed.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f367b2",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "- If distributions are reasonable, export the final master format and optionally enable `WRITE_FULL_MASTER=True` in `scripts/phase4_combine/create_master_training_file.py` and re-run.\n",
    "- Save plots and an imputed row list to `reports/imputation/` for audit and reviewer sign-off."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
