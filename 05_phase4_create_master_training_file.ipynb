{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2824755c",
   "metadata": {},
   "source": [
    "# Phase 4: Create Master Training Dataset\n",
    "## Combine GoEmotions + Crisis + Non-Crisis Data\n",
    "\n",
    "This notebook creates a **reduced, balanced dataset for training multi-task BERT**.\n",
    "\n",
    "### What this dataset is for:\n",
    "- **Train BERT** on emotion classification (using GoEmotions labels)\n",
    "- **Train BERT** on crisis detection (using crisis_label)\n",
    "- Smaller dataset (~217K rows) for efficient training\n",
    "\n",
    "### What happens after BERT is trained:\n",
    "1. Apply trained BERT to **ORIGINAL FULL datasets** (1.5M+ non-crisis, 67K crisis)\n",
    "2. Extract emotion features for ALL tweets\n",
    "3. Use these features to create episodes & hourly aggregations for RL agent\n",
    "\n",
    "### Data Sources:\n",
    "- **GoEmotions**: 54K Reddit comments with labeled emotions (for BERT training)\n",
    "- **Crisis**: 67K crisis tweets (all kept)\n",
    "- **Non-Crisis**: ~96K sampled non-crisis tweets (reduced from 1.5M for balanced training)\n",
    "\n",
    "### Sampling Strategy:\n",
    "- Non-crisis data is randomly sampled with sports emphasis\n",
    "- Dataset is shuffled so rows are randomized (not grouped by source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9085155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378adf6",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f45d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "1. GoEmotions (with 13 emotions)...\n",
      "   ‚úì Loaded 54,263 rows\n",
      "   Columns: ['text', 'emotion_label', 'emotion_name', 'id', 'labels']\n",
      "\n",
      "2. Crisis data (with emotion columns)...\n",
      "   ‚úì Loaded 66,748 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'informativeness', 'created_at_imputed', 'created_at_imputed_method', 'emotion_label', 'emotion_name']\n",
      "\n",
      "3. Non-crisis data (with emotion columns)...\n",
      "   ‚úì Loaded 1,533,696 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'emotion_label', 'emotion_name']\n",
      "\n",
      "================================================================================\n",
      "Total rows to combine: 1,654,707\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "# Load GoEmotions with 13 emotions\n",
    "print(\"1. GoEmotions (with 13 emotions)...\")\n",
    "df_goemotions = pd.read_csv('goemotion_data/goemotions_with_13_emotions.csv')\n",
    "print(f\"   ‚úì Loaded {len(df_goemotions):,} rows\")\n",
    "print(f\"   Columns: {df_goemotions.columns.tolist()}\")\n",
    "\n",
    "# Load crisis data with emotion columns\n",
    "print(\"\\n2. Crisis data (with emotion columns)...\")\n",
    "df_crisis = pd.read_csv('standardized_data/crisis_combined_with_emotions.csv')\n",
    "print(f\"   ‚úì Loaded {len(df_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_crisis.columns.tolist()}\")\n",
    "\n",
    "# Load non-crisis data with emotion columns\n",
    "print(\"\\n3. Non-crisis data (with emotion columns)...\")\n",
    "df_non_crisis = pd.read_csv('standardized_data/non_crisis_combined_with_emotions.csv')\n",
    "print(f\"   ‚úì Loaded {len(df_non_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_non_crisis.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total rows to combine: {len(df_goemotions) + len(df_crisis) + len(df_non_crisis):,}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ogmggp72bb",
   "metadata": {},
   "source": [
    "## 1.5 Sample Non-Crisis Data (Reduce Dataset Size)\n",
    "\n",
    "Downsample non-crisis data from 1.5M to ~100K rows:\n",
    "- Keep all GoEmotions (54K) - needed for training\n",
    "- Keep all Crisis (67K) - core data for crisis detection\n",
    "- Sample non-crisis to ~100K with sports emphasis\n",
    "\n",
    "**Sampling Distribution:**\n",
    "| Source | Type | Sample Size |\n",
    "|--------|------|-------------|\n",
    "| worldcup_2018 | Sports | 20,000 |\n",
    "| tokyo_olympics | Sports | 20,000 |\n",
    "| fifa_worldcup | Sports | 20,000 |\n",
    "| game_of_thrones | Entertainment | 20,000 |\n",
    "| us_election | Politics | 10,000 |\n",
    "| coachella | Entertainment | All (~3,846) |\n",
    "| music_concerts | Entertainment | All (~1,830) |\n",
    "\n",
    "**Rationale:** Sports-heavy distribution helps model learn to distinguish sports excitement from crisis fear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dforzd1aoro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling non-crisis data...\n",
      "\n",
      "Original non-crisis distribution:\n",
      "source_dataset\n",
      "game_of_thrones    760614\n",
      "worldcup_2018      458533\n",
      "tokyo_olympics     159432\n",
      "us_election         99948\n",
      "fifa_worldcup       49493\n",
      "coachella            3846\n",
      "music_concerts       1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total before sampling: 1,533,696\n",
      "  worldcup_2018: 20,000 (sampled from 458,533)\n",
      "  tokyo_olympics: 20,000 (sampled from 159,432)\n",
      "  fifa_worldcup: 20,000 (sampled from 49,493)\n",
      "  game_of_thrones: 20,000 (sampled from 760,614)\n",
      "  us_election: 10,000 (sampled from 99,948)\n",
      "  coachella: 3,846 (kept all)\n",
      "  music_concerts: 1,830 (kept all)\n",
      "\n",
      "============================================================\n",
      "Non-crisis sampling complete!\n",
      "  Before: 1,533,696 rows\n",
      "  After:  95,676 rows\n",
      "  Reduction: 93.8%\n",
      "============================================================\n",
      "\n",
      "New non-crisis distribution:\n",
      "source_dataset\n",
      "worldcup_2018      20000\n",
      "tokyo_olympics     20000\n",
      "fifa_worldcup      20000\n",
      "game_of_thrones    20000\n",
      "us_election        10000\n",
      "coachella           3846\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampling non-crisis data...\\n\")\n",
    "\n",
    "# Define sampling configuration\n",
    "SAMPLE_CONFIG = {\n",
    "    'worldcup_2018': 20000,      # Sports\n",
    "    'tokyo_olympics': 20000,     # Sports\n",
    "    'fifa_worldcup': 20000,      # Sports\n",
    "    'game_of_thrones': 20000,    # Entertainment\n",
    "    'us_election': 10000,        # Politics\n",
    "    'coachella': None,           # Keep all (small dataset)\n",
    "    'music_concerts': None,      # Keep all (small dataset)\n",
    "}\n",
    "\n",
    "print(\"Original non-crisis distribution:\")\n",
    "print(df_non_crisis['source_dataset'].value_counts())\n",
    "print(f\"\\nTotal before sampling: {len(df_non_crisis):,}\")\n",
    "\n",
    "# Sample each source\n",
    "sampled_dfs = []\n",
    "for source, sample_size in SAMPLE_CONFIG.items():\n",
    "    source_df = df_non_crisis[df_non_crisis['source_dataset'] == source]\n",
    "    \n",
    "    if sample_size is None or len(source_df) <= sample_size:\n",
    "        # Keep all rows for small datasets\n",
    "        sampled_dfs.append(source_df)\n",
    "        print(f\"  {source}: {len(source_df):,} (kept all)\")\n",
    "    else:\n",
    "        # Random sample for large datasets\n",
    "        sampled = source_df.sample(n=sample_size, random_state=42)\n",
    "        sampled_dfs.append(sampled)\n",
    "        print(f\"  {source}: {sample_size:,} (sampled from {len(source_df):,})\")\n",
    "\n",
    "# Combine sampled data\n",
    "df_non_crisis_sampled = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Non-crisis sampling complete!\")\n",
    "print(f\"  Before: {len(df_non_crisis):,} rows\")\n",
    "print(f\"  After:  {len(df_non_crisis_sampled):,} rows\")\n",
    "print(f\"  Reduction: {(1 - len(df_non_crisis_sampled)/len(df_non_crisis))*100:.1f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Replace original with sampled version\n",
    "df_non_crisis = df_non_crisis_sampled\n",
    "\n",
    "print(\"\\nNew non-crisis distribution:\")\n",
    "print(df_non_crisis['source_dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bdfd9",
   "metadata": {},
   "source": [
    "## 2. Check Current Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b68ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current column schemas:\n",
      "\n",
      "GoEmotions columns:\n",
      "  - text: str\n",
      "  - emotion_label: int64\n",
      "  - emotion_name: str\n",
      "  - id: str\n",
      "  - labels: str\n",
      "\n",
      "Crisis columns:\n",
      "  - text: str\n",
      "  - created_at: str\n",
      "  - event_name: str\n",
      "  - event_type: str\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: str\n",
      "  - informativeness: str\n",
      "  - created_at_imputed: bool\n",
      "  - created_at_imputed_method: float64\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n",
      "\n",
      "Non-crisis columns:\n",
      "  - text: str\n",
      "  - created_at: str\n",
      "  - event_name: str\n",
      "  - event_type: str\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: str\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Current column schemas:\\n\")\n",
    "\n",
    "print(\"GoEmotions columns:\")\n",
    "for col in df_goemotions.columns:\n",
    "    print(f\"  - {col}: {df_goemotions[col].dtype}\")\n",
    "\n",
    "print(\"\\nCrisis columns:\")\n",
    "for col in df_crisis.columns:\n",
    "    print(f\"  - {col}: {df_crisis[col].dtype}\")\n",
    "\n",
    "print(\"\\nNon-crisis columns:\")\n",
    "for col in df_non_crisis.columns:\n",
    "    print(f\"  - {col}: {df_non_crisis[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d6cb2",
   "metadata": {},
   "source": "## 3. Define Master Schema\n\nCreate unified column structure for all datasets:\n- **text**: Tweet/comment text\n- **emotion_label**: Numeric emotion (1-13, NULL for unlabeled)\n- **emotion_name**: Text emotion name (NULL for unlabeled)\n- **source_dataset**: Origin of data (GoEmotions, HumAID, CrisisLex, etc.)\n- **crisis_label**: Binary (1=crisis, 0=non-crisis, NULL for GoEmotions)\n- **event_type**: General category (hurricane, sports, etc., NULL for GoEmotions)\n- **event_name**: Specific event (hurricane_harvey_2017, etc., NULL for GoEmotions)\n- **informativeness**: CrisisLex informativeness label (NULL for others)\n\n**Note**: `created_at` is NOT included - BERT doesn't need timestamps. Timestamps are only needed for RL training, which uses the original full datasets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403345da",
   "metadata": {},
   "outputs": [],
   "source": "# Define master column set (NO created_at - not needed for BERT training)\nMASTER_COLUMNS = [\n    'text',\n    'emotion_label',\n    'emotion_name',\n    'source_dataset',\n    'crisis_label',\n    'event_type',\n    'event_name',\n    'informativeness'\n]\n\nprint(\"Master schema columns:\")\nfor i, col in enumerate(MASTER_COLUMNS, 1):\n    print(f\"  {i}. {col}\")"
  },
  {
   "cell_type": "markdown",
   "id": "c5ec4a07",
   "metadata": {},
   "source": [
    "## 4. Standardize GoEmotions Data\n",
    "\n",
    "Add missing columns to GoEmotions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e49b7",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Standardizing GoEmotions data...\\n\")\n\n# Create standardized GoEmotions dataframe\ndf_goemotions_std = pd.DataFrame()\n\n# Keep existing columns\ndf_goemotions_std['text'] = df_goemotions['text']\ndf_goemotions_std['emotion_label'] = df_goemotions['emotion_label']\ndf_goemotions_std['emotion_name'] = df_goemotions['emotion_name']\n\n# Add source\ndf_goemotions_std['source_dataset'] = 'GoEmotions'\n\n# Add NULL columns (GoEmotions is not crisis-related)\ndf_goemotions_std['crisis_label'] = np.nan\ndf_goemotions_std['event_type'] = ''\ndf_goemotions_std['event_name'] = ''\ndf_goemotions_std['informativeness'] = ''\n\nprint(f\"‚úì GoEmotions standardized: {len(df_goemotions_std):,} rows\")\nprint(f\"  Columns: {df_goemotions_std.columns.tolist()}\")\nprint(f\"\\nSample:\")\ndisplay(df_goemotions_std.head(3))"
  },
  {
   "cell_type": "markdown",
   "id": "e2cfc574",
   "metadata": {},
   "source": [
    "## 5. Standardize Crisis Data\n",
    "\n",
    "Select and reorder crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969d83b",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Standardizing crisis data...\\n\")\n\n# Create standardized crisis dataframe\ndf_crisis_std = pd.DataFrame()\n\ndf_crisis_std['text'] = df_crisis['text']\ndf_crisis_std['emotion_label'] = df_crisis['emotion_label']  # Will be NaN\ndf_crisis_std['emotion_name'] = df_crisis['emotion_name']    # Will be empty\ndf_crisis_std['source_dataset'] = df_crisis['source_dataset']\ndf_crisis_std['crisis_label'] = df_crisis['crisis_label']\ndf_crisis_std['event_type'] = df_crisis['event_type']\ndf_crisis_std['event_name'] = df_crisis['event_name']\ndf_crisis_std['informativeness'] = df_crisis['informativeness']\n\nprint(f\"‚úì Crisis standardized: {len(df_crisis_std):,} rows\")\nprint(f\"  Columns: {df_crisis_std.columns.tolist()}\")\nprint(f\"\\nSample:\")\ndisplay(df_crisis_std.head(3))"
  },
  {
   "cell_type": "markdown",
   "id": "e702522d",
   "metadata": {},
   "source": [
    "## 6. Standardize Non-Crisis Data\n",
    "\n",
    "Select and reorder non-crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd741cd",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Standardizing non-crisis data...\\n\")\n\n# Create standardized non-crisis dataframe\ndf_non_crisis_std = pd.DataFrame()\n\ndf_non_crisis_std['text'] = df_non_crisis['text']\ndf_non_crisis_std['emotion_label'] = df_non_crisis['emotion_label']  # Will be NaN\ndf_non_crisis_std['emotion_name'] = df_non_crisis['emotion_name']    # Will be empty\ndf_non_crisis_std['source_dataset'] = df_non_crisis['source_dataset']\ndf_non_crisis_std['crisis_label'] = df_non_crisis['crisis_label']\ndf_non_crisis_std['event_type'] = df_non_crisis['event_type']\ndf_non_crisis_std['event_name'] = df_non_crisis['event_name']\n\n# Non-crisis doesn't have informativeness\ndf_non_crisis_std['informativeness'] = ''\n\nprint(f\"‚úì Non-crisis standardized: {len(df_non_crisis_std):,} rows\")\nprint(f\"  Columns: {df_non_crisis_std.columns.tolist()}\")\nprint(f\"\\nSample:\")\ndisplay(df_non_crisis_std.head(3))"
  },
  {
   "cell_type": "markdown",
   "id": "f7e3fcb7",
   "metadata": {},
   "source": [
    "## 7. Validate Schema Alignment\n",
    "\n",
    "Ensure all three datasets have identical column structure before combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec744ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SCHEMA VALIDATION\n",
      "================================================================================\n",
      "\n",
      "GoEmotions columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "Crisis columns:     ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "Non-crisis columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "‚úÖ All datasets have matching column structure!\n",
      "\n",
      "‚úÖ Columns match master schema!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SCHEMA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check column names\n",
    "goemotions_cols = df_goemotions_std.columns.tolist()\n",
    "crisis_cols = df_crisis_std.columns.tolist()\n",
    "non_crisis_cols = df_non_crisis_std.columns.tolist()\n",
    "\n",
    "print(f\"\\nGoEmotions columns: {goemotions_cols}\")\n",
    "print(f\"Crisis columns:     {crisis_cols}\")\n",
    "print(f\"Non-crisis columns: {non_crisis_cols}\")\n",
    "\n",
    "# Validate all match\n",
    "if goemotions_cols == crisis_cols == non_crisis_cols:\n",
    "    print(\"\\n‚úÖ All datasets have matching column structure!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Column mismatch detected!\")\n",
    "    print(f\"\\nDifferences:\")\n",
    "    if goemotions_cols != crisis_cols:\n",
    "        print(f\"  GoEmotions vs Crisis: {set(goemotions_cols) ^ set(crisis_cols)}\")\n",
    "    if crisis_cols != non_crisis_cols:\n",
    "        print(f\"  Crisis vs Non-crisis: {set(crisis_cols) ^ set(non_crisis_cols)}\")\n",
    "\n",
    "# Check if columns match master schema\n",
    "if goemotions_cols == MASTER_COLUMNS:\n",
    "    print(\"\\n‚úÖ Columns match master schema!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Column order differs from master schema\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add23f7",
   "metadata": {},
   "source": [
    "## 8. Combine All Datasets\n",
    "\n",
    "Concatenate all three standardized datasets into master training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba252696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "\n",
      "Combined master dataset: 216,687 rows\n",
      "Shuffling dataset to randomize row order...\n",
      "\n",
      "‚úÖ Combined and shuffled master dataset created!\n",
      "\n",
      "Total rows: 216,687\n",
      "\n",
      "Breakdown:\n",
      "  GoEmotions:  54,263 (25.0%)\n",
      "  Crisis:      66,748 (30.8%)\n",
      "  Non-crisis:  95,676 (44.2%)\n",
      "\n",
      "Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Memory usage: 115.94 MB\n",
      "\n",
      "First 10 rows source distribution (showing shuffle worked):\n",
      "['tokyo_olympics', 'worldcup_2018', 'humaid', 'humaid', 'tokyo_olympics', 'humaid', 'GoEmotions', 'humaid', 'GoEmotions', 'GoEmotions']\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining datasets...\\n\")\n",
    "\n",
    "# Concatenate all datasets\n",
    "df_master = pd.concat([\n",
    "    df_goemotions_std,\n",
    "    df_crisis_std,\n",
    "    df_non_crisis_std\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Combined master dataset: {len(df_master):,} rows\")\n",
    "\n",
    "# Shuffle the dataset so rows are randomized (not grouped by source)\n",
    "print(\"Shuffling dataset to randomize row order...\")\n",
    "df_master = df_master.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Combined and shuffled master dataset created!\")\n",
    "print(f\"\\nTotal rows: {len(df_master):,}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  GoEmotions:  {len(df_goemotions_std):,} ({len(df_goemotions_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Crisis:      {len(df_crisis_std):,} ({len(df_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Non-crisis:  {len(df_non_crisis_std):,} ({len(df_non_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nColumns: {df_master.columns.tolist()}\")\n",
    "print(f\"\\nMemory usage: {df_master.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "# Show that rows are now mixed\n",
    "print(f\"\\nFirst 10 rows source distribution (showing shuffle worked):\")\n",
    "print(df_master.head(10)['source_dataset'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d310a",
   "metadata": {},
   "source": [
    "## 9. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b43a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Null counts:\n",
      "text                   25\n",
      "emotion_label      162424\n",
      "emotion_name       162424\n",
      "source_dataset          0\n",
      "crisis_label        54263\n",
      "event_type              0\n",
      "event_name              0\n",
      "created_at              0\n",
      "informativeness     43816\n",
      "dtype: int64\n",
      "\n",
      "Text validation:\n",
      "  Null texts: 25\n",
      "  Empty texts: 0\n",
      "\n",
      "Emotion label status:\n",
      "  Labeled (GoEmotions):    54,263 (25.0%)\n",
      "  Unlabeled (Crisis+Non):  162,424 (75.0%)\n",
      "\n",
      "Crisis label distribution:\n",
      "  Crisis (1):      66,748\n",
      "  Non-crisis (0):  95,676\n",
      "  Unlabeled (GoE): 54,263\n",
      "\n",
      "Source dataset distribution:\n",
      "source_dataset\n",
      "GoEmotions         54263\n",
      "humaid             43409\n",
      "crisislex          23339\n",
      "tokyo_olympics     20000\n",
      "worldcup_2018      20000\n",
      "fifa_worldcup      20000\n",
      "game_of_thrones    20000\n",
      "us_election        10000\n",
      "coachella           3846\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "print(f\"\\nNull counts:\")\n",
    "print(df_master.isnull().sum())\n",
    "\n",
    "# Check text column\n",
    "null_text = df_master['text'].isna().sum()\n",
    "empty_text = (df_master['text'] == '').sum()\n",
    "print(f\"\\nText validation:\")\n",
    "print(f\"  Null texts: {null_text}\")\n",
    "print(f\"  Empty texts: {empty_text}\")\n",
    "if null_text == 0 and empty_text == 0:\n",
    "    print(f\"  ‚úÖ All rows have text content\")\n",
    "\n",
    "# Check emotion labels\n",
    "labeled_rows = df_master['emotion_label'].notna().sum()\n",
    "unlabeled_rows = df_master['emotion_label'].isna().sum()\n",
    "print(f\"\\nEmotion label status:\")\n",
    "print(f\"  Labeled (GoEmotions):    {labeled_rows:,} ({labeled_rows/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Unlabeled (Crisis+Non):  {unlabeled_rows:,} ({unlabeled_rows/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "# Check crisis labels\n",
    "crisis_rows = (df_master['crisis_label'] == 1).sum()\n",
    "non_crisis_rows = (df_master['crisis_label'] == 0).sum()\n",
    "unlabeled_crisis = df_master['crisis_label'].isna().sum()\n",
    "print(f\"\\nCrisis label distribution:\")\n",
    "print(f\"  Crisis (1):      {crisis_rows:,}\")\n",
    "print(f\"  Non-crisis (0):  {non_crisis_rows:,}\")\n",
    "print(f\"  Unlabeled (GoE): {unlabeled_crisis:,}\")\n",
    "\n",
    "# Check source distribution\n",
    "print(f\"\\nSource dataset distribution:\")\n",
    "print(df_master['source_dataset'].value_counts())\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3e5d6",
   "metadata": {},
   "source": [
    "## 10. Show Sample Data from Each Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e09420da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from each source:\n",
      "\n",
      "GoEmotions sample (with emotion labels):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Xanax thing is burning itself out.Crims don't give a fuck.</td>\n",
       "      <td>13.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's doing stupid things when you're young. Then there's doing horribly stupid things when yo...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>anger</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>One of my favs was when we were in the playoffs against the Habs and we won all five fights in a...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "6                                       The Xanax thing is burning itself out.Crims don't give a fuck.   \n",
       "8  There's doing stupid things when you're young. Then there's doing horribly stupid things when yo...   \n",
       "9  One of my favs was when we were in the playoffs against the Habs and we won all five fights in a...   \n",
       "\n",
       "   emotion_label emotion_name source_dataset  \n",
       "6           13.0      neutral     GoEmotions  \n",
       "8            2.0        anger     GoEmotions  \n",
       "9           13.0      neutral     GoEmotions  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Watters: Trump Bashed by Left After Obama Golfed During LA Floods | Fox News Insider</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hurricane_harvey_2017_train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Texas: ExxonMobile tank damaged by Hurricane Harvey leaking dangerous pollutants&nbsp;&nbsp;#HurricaneHar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hurricane_harvey_2017_train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Australia is stepping up its assistance to New Zealand after the deadly earthquake on the countr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kaikoura_earthquake_2016_test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "2                 Watters: Trump Bashed by Left After Obama Golfed During LA Floods | Fox News Insider   \n",
       "3  #Texas: ExxonMobile tank damaged by Hurricane Harvey leaking dangerous pollutants  #HurricaneHar...   \n",
       "5  Australia is stepping up its assistance to New Zealand after the deadly earthquake on the countr...   \n",
       "\n",
       "   emotion_label emotion_name                     event_name  crisis_label  \n",
       "2            NaN          NaN    hurricane_harvey_2017_train           1.0  \n",
       "3            NaN          NaN    hurricane_harvey_2017_train           1.0  \n",
       "5            NaN          NaN  kaikoura_earthquake_2016_test           1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Made The √∞≈∏‚Ä°¬Æ√∞≈∏‚Ä°¬≥ Flag Fly High In Tokyo And We Are Proud Of You! √¢¬ù¬§√Ø¬∏¬è\\n #MirabaiChanu!\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tokyo_olympics_2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signs of the World Cup learn to sign goal in British Sign Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BH DtL√∞≈∏‚Äú¬£√∞≈∏‚Äú¬£and FH cross winners.\\nGotcha 1S. Vamooooooooooooos\\n@keinishikori \\n#Tokyo2020 #T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tokyo_olympics_2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  You Made The √∞≈∏‚Ä°¬Æ√∞≈∏‚Ä°¬≥ Flag Fly High In Tokyo And We Are Proud Of You! √¢¬ù¬§√Ø¬∏¬è\\n #MirabaiChanu!\\n\\...   \n",
       "1                                   signs of the World Cup learn to sign goal in British Sign Language   \n",
       "4  BH DtL√∞≈∏‚Äú¬£√∞≈∏‚Äú¬£and FH cross winners.\\nGotcha 1S. Vamooooooooooooos\\n@keinishikori \\n#Tokyo2020 #T...   \n",
       "\n",
       "   emotion_label emotion_name           event_name  crisis_label  \n",
       "0            NaN          NaN  tokyo_olympics_2020           0.0  \n",
       "1            NaN          NaN   fifa_worldcup_2018           0.0  \n",
       "4            NaN          NaN  tokyo_olympics_2020           0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample rows from each source:\\n\")\n",
    "\n",
    "print(\"GoEmotions sample (with emotion labels):\")\n",
    "display(df_master[df_master['source_dataset'] == 'GoEmotions'][['text', 'emotion_label', 'emotion_name', 'source_dataset']].head(3))\n",
    "\n",
    "print(\"\\nCrisis sample (emotion labels = NULL):\")\n",
    "crisis_sample = df_master[df_master['crisis_label'] == 1][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(crisis_sample)\n",
    "\n",
    "print(\"\\nNon-crisis sample (emotion labels = NULL):\")\n",
    "non_crisis_sample = df_master[df_master['crisis_label'] == 0][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(non_crisis_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e489f01",
   "metadata": {},
   "source": [
    "## 11. Save Master Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0010022",
   "metadata": {},
   "outputs": [],
   "source": "# Save to master_training_data folder\noutput_path = 'master_training_data/master_training_data_v3.csv'\n\nprint(f\"Saving master training dataset to {output_path}...\\n\")\n\ndf_master.to_csv(output_path, index=False)\n\nfile_size = Path(output_path).stat().st_size / (1024**2)\n\nprint(\"=\" * 80)\nprint(\"MASTER DATASET SAVED\")\nprint(\"=\" * 80)\nprint(f\"\\n‚úÖ Saved to: {output_path}\")\nprint(f\"\\nFile size: {file_size:.2f} MB\")\nprint(f\"Total rows: {len(df_master):,}\")\nprint(f\"Total columns: {len(df_master.columns)}\")\nprint(f\"\\nColumns: {df_master.columns.tolist()}\")\nprint(f\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "689328d9",
   "metadata": {},
   "source": [
    "## 12. Create Smaller Sample File for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce99657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created sample file: master_training_data/master_training_sample_10kv2.csv\n",
      "   Rows: 10,000\n",
      "   Size: 1.67 MB\n"
     ]
    }
   ],
   "source": [
    "# Create 10K sample for quick testing\n",
    "sample_size = 10000\n",
    "df_sample = df_master.sample(n=sample_size, random_state=42)\n",
    "\n",
    "sample_path = 'master_training_data/master_training_sample_10kv2.csv'\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Created sample file: {sample_path}\")\n",
    "print(f\"   Rows: {len(df_sample):,}\")\n",
    "print(f\"   Size: {Path(sample_path).stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ee71",
   "metadata": {},
   "source": [
    "## 13. Final Summary & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67521c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Composition:\n",
      "   Total rows:          216,687\n",
      "   GoEmotions:          54,263 (with emotion labels)\n",
      "   Crisis events:       66,748 (emotion labels = NULL)\n",
      "   Non-crisis events:   95,676 (SAMPLED, emotion labels = NULL)\n",
      "\n",
      "üìÅ Files Created:\n",
      "   Main:   master_training_data/master_training_data_v3.csv (36.08 MB)\n",
      "   Sample: master_training_data/master_training_sample_10k.csv\n",
      "\n",
      "üè∑Ô∏è Emotion Labels:\n",
      "   Labeled rows:    54,263 (GoEmotions - for training)\n",
      "   Unlabeled rows:  162,424 (Crisis + Non-crisis - for prediction)\n",
      "\n",
      "üîß Schema:\n",
      "   Columns: 9\n",
      "      1. text\n",
      "      2. emotion_label\n",
      "      3. emotion_name\n",
      "      4. source_dataset\n",
      "      5. crisis_label\n",
      "      6. event_type\n",
      "      7. event_name\n",
      "      8. created_at\n",
      "      9. informativeness\n",
      "\n",
      "üìã Next Steps:\n",
      "   1. Train multi-task BERT on this dataset (~217K rows)\n",
      "      - Task 1: Emotion classification (using GoEmotions labels)\n",
      "      - Task 2: Crisis detection (using crisis_label)\n",
      "   2. Apply trained BERT to ORIGINAL FULL datasets:\n",
      "      - Full crisis data: 67K tweets\n",
      "      - Full non-crisis data: 1.5M+ tweets\n",
      "   3. Extract emotion features for ALL tweets\n",
      "   4. Create episodes & hourly aggregations for RL agent\n",
      "   5. Train RL agent on temporal emotion patterns\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PHASE 4 COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Composition:\")\n",
    "print(f\"   Total rows:          {len(df_master):,}\")\n",
    "print(f\"   GoEmotions:          {len(df_goemotions_std):,} (with emotion labels)\")\n",
    "print(f\"   Crisis events:       {len(df_crisis_std):,} (emotion labels = NULL)\")\n",
    "print(f\"   Non-crisis events:   {len(df_non_crisis_std):,} (SAMPLED, emotion labels = NULL)\")\n",
    "\n",
    "print(f\"\\nüìÅ Files Created:\")\n",
    "print(f\"   Main:   master_training_data/master_training_data_v3.csv ({file_size:.2f} MB)\")\n",
    "print(f\"   Sample: master_training_data/master_training_sample_10k.csv\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Emotion Labels:\")\n",
    "print(f\"   Labeled rows:    {labeled_rows:,} (GoEmotions - for training)\")\n",
    "print(f\"   Unlabeled rows:  {unlabeled_rows:,} (Crisis + Non-crisis - for prediction)\")\n",
    "\n",
    "print(f\"\\nüîß Schema:\")\n",
    "print(f\"   Columns: {len(df_master.columns)}\")\n",
    "for i, col in enumerate(df_master.columns, 1):\n",
    "    print(f\"      {i}. {col}\")\n",
    "\n",
    "print(f\"\\nüìã Next Steps:\")\n",
    "print(f\"   1. Train multi-task BERT on this dataset (~217K rows)\")\n",
    "print(f\"      - Task 1: Emotion classification (using GoEmotions labels)\")\n",
    "print(f\"      - Task 2: Crisis detection (using crisis_label)\")\n",
    "print(f\"   2. Apply trained BERT to ORIGINAL FULL datasets:\")\n",
    "print(f\"      - Full crisis data: 67K tweets\")\n",
    "print(f\"      - Full non-crisis data: 1.5M+ tweets\")\n",
    "print(f\"   3. Extract emotion features for ALL tweets\")\n",
    "print(f\"   4. Create episodes & hourly aggregations for RL agent\")\n",
    "print(f\"   5. Train RL agent on temporal emotion patterns\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PHASE 4 COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}