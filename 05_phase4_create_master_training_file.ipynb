{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2824755c",
   "metadata": {},
   "source": [
    "# Phase 4: Create Master Training Dataset\n",
    "## Combine GoEmotions + Crisis + Non-Crisis Data\n",
    "\n",
    "This notebook creates a **stratified, balanced 60K dataset for training multi-task BERT**.\n",
    "\n",
    "### Target Dataset Composition (60K total):\n",
    "\n",
    "| Source | Target Rows | Stratification |\n",
    "|--------|-------------|----------------|\n",
    "| **Crisis** | 25,000 | Balanced by event_type |\n",
    "| **Non-Crisis** | 18,000 | Balanced by source_dataset |\n",
    "| **GoEmotions** | 17,000 | Balanced by emotion_label (13 emotions) |\n",
    "\n",
    "### Why Stratified Sampling?\n",
    "- Ensures all 13 emotions are equally represented\n",
    "- Ensures all crisis event types (hurricane, wildfire, etc.) are represented\n",
    "- Ensures balanced crisis vs non-crisis for classification\n",
    "- Smaller dataset (60K) is more efficient for BERT training\n",
    "\n",
    "### What happens after BERT is trained:\n",
    "1. Apply trained BERT to **ORIGINAL FULL datasets** (1.5M+ non-crisis, 67K crisis)\n",
    "2. Extract emotion features for ALL tweets\n",
    "3. Use these features to create episodes & hourly aggregations for RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9085155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378adf6",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f45d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "1. GoEmotions (with 13 emotions)...\n",
      "   ‚úì Loaded 54,263 rows\n",
      "   Columns: ['text', 'emotion_label', 'emotion_name', 'id', 'labels']\n",
      "\n",
      "2. Crisis data (with emotion columns)...\n",
      "   ‚úì Loaded 66,748 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'informativeness', 'emotion_label', 'emotion_name']\n",
      "\n",
      "3. Non-crisis data (with emotion columns)...\n",
      "   ‚úì Loaded 1,533,696 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'emotion_label', 'emotion_name']\n",
      "\n",
      "================================================================================\n",
      "Total rows to combine: 1,654,707\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "# Load GoEmotions with 13 emotions\n",
    "print(\"1. GoEmotions (with 13 emotions)...\")\n",
    "df_goemotions = pd.read_csv('goemotion_data/goemotions_with_13_emotions.csv')\n",
    "print(f\"   ‚úì Loaded {len(df_goemotions):,} rows\")\n",
    "print(f\"   Columns: {df_goemotions.columns.tolist()}\")\n",
    "\n",
    "# Load crisis data with emotion columns\n",
    "print(\"\\n2. Crisis data (with emotion columns)...\")\n",
    "df_crisis = pd.read_csv('standardized_data/crisis_combined_with_emotions.csv')\n",
    "print(f\"   ‚úì Loaded {len(df_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_crisis.columns.tolist()}\")\n",
    "\n",
    "# Load non-crisis data with emotion columns\n",
    "print(\"\\n3. Non-crisis data (with emotion columns)...\")\n",
    "df_non_crisis = pd.read_csv('standardized_data/non_crisis_combined_with_emotions.csv')\n",
    "print(f\"   ‚úì Loaded {len(df_non_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_non_crisis.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total rows to combine: {len(df_goemotions) + len(df_crisis) + len(df_non_crisis):,}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ogmggp72bb",
   "metadata": {},
   "source": [
    "## 1.5 Stratified Sampling (Create 60K Balanced Dataset)\n",
    "\n",
    "Apply stratified sampling to create a balanced dataset:\n",
    "\n",
    "| Source | Target | Strategy |\n",
    "|--------|--------|----------|\n",
    "| **Crisis** | 25,000 | Equal samples per event_type |\n",
    "| **Non-Crisis** | 18,000 | Equal samples per source_dataset |\n",
    "| **GoEmotions** | 17,000 | Equal samples per emotion (~1,308 per emotion) |\n",
    "\n",
    "This ensures:\n",
    "- All 13 emotions are represented equally\n",
    "- All crisis event types are represented\n",
    "- All non-crisis sources are represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dforzd1aoro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATIFIED SAMPLING TO 60,000 ROWS\n",
      "================================================================================\n",
      "\n",
      "Targets:\n",
      "  Crisis:      25,000\n",
      "  Non-crisis:  18,000\n",
      "  GoEmotions:  17,000\n",
      "  TOTAL:       60,000\n",
      "\n",
      "------------------------------------------------------------\n",
      "1. Sampling GoEmotions - Equal across 13 emotions\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original GoEmotions: 54,263 rows\n",
      "Emotion distribution:\n",
      "emotion_label\n",
      "1       658\n",
      "2      4607\n",
      "3      1148\n",
      "4       132\n",
      "5      3753\n",
      "6      1831\n",
      "7      1044\n",
      "8      1218\n",
      "9     11048\n",
      "10     1543\n",
      "11     4093\n",
      "12     3898\n",
      "13    19290\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target per emotion: 1307\n",
      "  ‚ö†Ô∏è  Emotion 1: only 658 available\n",
      "  ‚ö†Ô∏è  Emotion 3: only 1148 available\n",
      "  ‚ö†Ô∏è  Emotion 4: only 132 available\n",
      "  ‚ö†Ô∏è  Emotion 7: only 1044 available\n",
      "  ‚ö†Ô∏è  Emotion 8: only 1218 available\n",
      "\n",
      "‚úÖ GoEmotions sampled: 14,656 rows\n",
      "\n",
      "------------------------------------------------------------\n",
      "2. Sampling Crisis - Balanced by event_type\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original Crisis: 66,748 rows\n",
      "Event type distribution:\n",
      "event_type\n",
      "hurricane     33422\n",
      "earthquake    11429\n",
      "flood          7498\n",
      "wildfire       7151\n",
      "accident       4248\n",
      "bombing        2000\n",
      "haze           1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique event types: 7\n",
      "Target per event type: 3571\n",
      "  ‚ö†Ô∏è  bombing: only 2000 available\n",
      "  ‚ö†Ô∏è  haze: only 1000 available\n",
      "\n",
      "‚úÖ Crisis sampled: 20,855 rows\n",
      "\n",
      "------------------------------------------------------------\n",
      "3. Sampling Non-Crisis - Balanced by source_dataset\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original Non-Crisis: 1,533,696 rows\n",
      "Source distribution:\n",
      "source_dataset\n",
      "game_of_thrones    760614\n",
      "worldcup_2018      458533\n",
      "tokyo_olympics     159432\n",
      "us_election         99948\n",
      "fifa_worldcup       49493\n",
      "coachella            3846\n",
      "music_concerts       1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique sources: 7\n",
      "Target per source: 2571\n",
      "  ‚ö†Ô∏è  music_concerts: only 1830 available\n",
      "\n",
      "‚úÖ Non-Crisis sampled: 17,256 rows\n",
      "\n",
      "================================================================================\n",
      "STRATIFIED SAMPLING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä Final counts:\n",
      "   GoEmotions:  14,656 rows\n",
      "   Crisis:      20,855 rows\n",
      "   Non-crisis:  17,256 rows\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   TOTAL:       52,767 rows\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STRATIFIED SAMPLING CONFIGURATION\n",
    "# =============================================================================\n",
    "TARGET_CRISIS = 25000\n",
    "TARGET_NON_CRISIS = 18000\n",
    "TARGET_GOEMOTIONS = 17000\n",
    "TARGET_TOTAL = TARGET_CRISIS + TARGET_NON_CRISIS + TARGET_GOEMOTIONS  # 60K\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"STRATIFIED SAMPLING TO {TARGET_TOTAL:,} ROWS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTargets:\")\n",
    "print(f\"  Crisis:      {TARGET_CRISIS:,}\")\n",
    "print(f\"  Non-crisis:  {TARGET_NON_CRISIS:,}\")\n",
    "print(f\"  GoEmotions:  {TARGET_GOEMOTIONS:,}\")\n",
    "print(f\"  TOTAL:       {TARGET_TOTAL:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. STRATIFIED SAMPLE FROM GOEMOTIONS (by emotion_label)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"1. Sampling GoEmotions - Equal across 13 emotions\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nOriginal GoEmotions: {len(df_goemotions):,} rows\")\n",
    "print(\"Emotion distribution:\")\n",
    "print(df_goemotions['emotion_label'].value_counts().sort_index())\n",
    "\n",
    "n_emotions = 13\n",
    "target_per_emotion = TARGET_GOEMOTIONS // n_emotions  # ~1308 per emotion\n",
    "print(f\"\\nTarget per emotion: {target_per_emotion}\")\n",
    "\n",
    "goemotions_sampled = []\n",
    "for emotion_label in range(1, 14):\n",
    "    emotion_df = df_goemotions[df_goemotions['emotion_label'] == emotion_label]\n",
    "    available = len(emotion_df)\n",
    "    \n",
    "    if available >= target_per_emotion:\n",
    "        sampled = emotion_df.sample(n=target_per_emotion, random_state=42)\n",
    "    else:\n",
    "        sampled = emotion_df\n",
    "        print(f\"  ‚ö†Ô∏è  Emotion {emotion_label}: only {available} available\")\n",
    "    \n",
    "    goemotions_sampled.append(sampled)\n",
    "\n",
    "df_goemotions = pd.concat(goemotions_sampled, ignore_index=True)\n",
    "print(f\"\\n‚úÖ GoEmotions sampled: {len(df_goemotions):,} rows\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. STRATIFIED SAMPLE FROM CRISIS (by event_type)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"2. Sampling Crisis - Balanced by event_type\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nOriginal Crisis: {len(df_crisis):,} rows\")\n",
    "print(\"Event type distribution:\")\n",
    "print(df_crisis['event_type'].value_counts())\n",
    "\n",
    "n_event_types = df_crisis['event_type'].nunique()\n",
    "target_per_event = TARGET_CRISIS // n_event_types\n",
    "print(f\"\\nUnique event types: {n_event_types}\")\n",
    "print(f\"Target per event type: {target_per_event}\")\n",
    "\n",
    "crisis_sampled = []\n",
    "for event_type in df_crisis['event_type'].unique():\n",
    "    event_df = df_crisis[df_crisis['event_type'] == event_type]\n",
    "    available = len(event_df)\n",
    "    \n",
    "    if available >= target_per_event:\n",
    "        sampled = event_df.sample(n=target_per_event, random_state=42)\n",
    "    else:\n",
    "        sampled = event_df\n",
    "        print(f\"  ‚ö†Ô∏è  {event_type}: only {available} available\")\n",
    "    \n",
    "    crisis_sampled.append(sampled)\n",
    "\n",
    "df_crisis = pd.concat(crisis_sampled, ignore_index=True)\n",
    "print(f\"\\n‚úÖ Crisis sampled: {len(df_crisis):,} rows\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. STRATIFIED SAMPLE FROM NON-CRISIS (by source_dataset)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"3. Sampling Non-Crisis - Balanced by source_dataset\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nOriginal Non-Crisis: {len(df_non_crisis):,} rows\")\n",
    "print(\"Source distribution:\")\n",
    "print(df_non_crisis['source_dataset'].value_counts())\n",
    "\n",
    "n_sources = df_non_crisis['source_dataset'].nunique()\n",
    "target_per_source = TARGET_NON_CRISIS // n_sources\n",
    "print(f\"\\nUnique sources: {n_sources}\")\n",
    "print(f\"Target per source: {target_per_source}\")\n",
    "\n",
    "non_crisis_sampled = []\n",
    "for source in df_non_crisis['source_dataset'].unique():\n",
    "    source_df = df_non_crisis[df_non_crisis['source_dataset'] == source]\n",
    "    available = len(source_df)\n",
    "    \n",
    "    if available >= target_per_source:\n",
    "        sampled = source_df.sample(n=target_per_source, random_state=42)\n",
    "    else:\n",
    "        sampled = source_df\n",
    "        print(f\"  ‚ö†Ô∏è  {source}: only {available} available\")\n",
    "    \n",
    "    non_crisis_sampled.append(sampled)\n",
    "\n",
    "df_non_crisis = pd.concat(non_crisis_sampled, ignore_index=True)\n",
    "print(f\"\\n‚úÖ Non-Crisis sampled: {len(df_non_crisis):,} rows\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STRATIFIED SAMPLING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Final counts:\")\n",
    "print(f\"   GoEmotions:  {len(df_goemotions):,} rows\")\n",
    "print(f\"   Crisis:      {len(df_crisis):,} rows\")\n",
    "print(f\"   Non-crisis:  {len(df_non_crisis):,} rows\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   TOTAL:       {len(df_goemotions) + len(df_crisis) + len(df_non_crisis):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bdfd9",
   "metadata": {},
   "source": [
    "## 2. Check Current Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b68ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current column schemas:\n",
      "\n",
      "GoEmotions columns:\n",
      "  - text: str\n",
      "  - emotion_label: int64\n",
      "  - emotion_name: str\n",
      "  - id: str\n",
      "  - labels: str\n",
      "\n",
      "Crisis columns:\n",
      "  - text: str\n",
      "  - created_at: str\n",
      "  - event_name: str\n",
      "  - event_type: str\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: str\n",
      "  - informativeness: str\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n",
      "\n",
      "Non-crisis columns:\n",
      "  - text: str\n",
      "  - created_at: str\n",
      "  - event_name: str\n",
      "  - event_type: str\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: str\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Current column schemas:\\n\")\n",
    "\n",
    "print(\"GoEmotions columns:\")\n",
    "for col in df_goemotions.columns:\n",
    "    print(f\"  - {col}: {df_goemotions[col].dtype}\")\n",
    "\n",
    "print(\"\\nCrisis columns:\")\n",
    "for col in df_crisis.columns:\n",
    "    print(f\"  - {col}: {df_crisis[col].dtype}\")\n",
    "\n",
    "print(\"\\nNon-crisis columns:\")\n",
    "for col in df_non_crisis.columns:\n",
    "    print(f\"  - {col}: {df_non_crisis[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d6cb2",
   "metadata": {},
   "source": [
    "## 3. Define Master Schema\n",
    "\n",
    "Create unified column structure for all datasets:\n",
    "- **text**: Tweet/comment text\n",
    "- **emotion_label**: Numeric emotion (1-13, NULL for unlabeled)\n",
    "- **emotion_name**: Text emotion name (NULL for unlabeled)\n",
    "- **source_dataset**: Origin of data (GoEmotions, HumAID, CrisisLex, etc.)\n",
    "- **crisis_label**: Binary (1=crisis, 0=non-crisis, NULL for GoEmotions)\n",
    "- **event_type**: General category (hurricane, sports, etc., NULL for GoEmotions)\n",
    "- **event_name**: Specific event (hurricane_harvey_2017, etc., NULL for GoEmotions)\n",
    "- **informativeness**: CrisisLex informativeness label (NULL for others)\n",
    "\n",
    "**Note**: `created_at` is NOT included - BERT doesn't need timestamps. Timestamps are only needed for RL training, which uses the original full datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403345da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master schema columns:\n",
      "  1. text\n",
      "  2. emotion_label\n",
      "  3. emotion_name\n",
      "  4. source_dataset\n",
      "  5. crisis_label\n",
      "  6. event_type\n",
      "  7. event_name\n",
      "  8. informativeness\n"
     ]
    }
   ],
   "source": [
    "# Define master column set (NO created_at - not needed for BERT training)\n",
    "MASTER_COLUMNS = [\n",
    "    'text',\n",
    "    'emotion_label',\n",
    "    'emotion_name',\n",
    "    'source_dataset',\n",
    "    'crisis_label',\n",
    "    'event_type',\n",
    "    'event_name',\n",
    "    'informativeness'\n",
    "]\n",
    "\n",
    "print(\"Master schema columns:\")\n",
    "for i, col in enumerate(MASTER_COLUMNS, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec4a07",
   "metadata": {},
   "source": [
    "## 4. Standardize GoEmotions Data\n",
    "\n",
    "Add missing columns to GoEmotions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311e49b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing GoEmotions data...\n",
      "\n",
      "‚úì GoEmotions standardized: 14,656 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your coaching is terrible.... be ready and see how [NAME] uses [NAME]</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He may have, I was more worried about the \"running and shooting the AR one handed, off to the si...</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                                                                          To make her feel threatened   \n",
       "1                                Your coaching is terrible.... be ready and see how [NAME] uses [NAME]   \n",
       "2  He may have, I was more worried about the \"running and shooting the AR one handed, off to the si...   \n",
       "\n",
       "   emotion_label emotion_name source_dataset  crisis_label event_type  \\\n",
       "0              1         fear     GoEmotions           NaN              \n",
       "1              1         fear     GoEmotions           NaN              \n",
       "2              1         fear     GoEmotions           NaN              \n",
       "\n",
       "  event_name informativeness  \n",
       "0                             \n",
       "1                             \n",
       "2                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing GoEmotions data...\\n\")\n",
    "\n",
    "# Create standardized GoEmotions dataframe\n",
    "df_goemotions_std = pd.DataFrame()\n",
    "\n",
    "# Keep existing columns\n",
    "df_goemotions_std['text'] = df_goemotions['text']\n",
    "df_goemotions_std['emotion_label'] = df_goemotions['emotion_label']\n",
    "df_goemotions_std['emotion_name'] = df_goemotions['emotion_name']\n",
    "\n",
    "# Add source\n",
    "df_goemotions_std['source_dataset'] = 'GoEmotions'\n",
    "\n",
    "# Add NULL columns (GoEmotions is not crisis-related)\n",
    "df_goemotions_std['crisis_label'] = np.nan\n",
    "df_goemotions_std['event_type'] = ''\n",
    "df_goemotions_std['event_name'] = ''\n",
    "df_goemotions_std['informativeness'] = ''\n",
    "\n",
    "print(f\"‚úì GoEmotions standardized: {len(df_goemotions_std):,} rows\")\n",
    "print(f\"  Columns: {df_goemotions_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_goemotions_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cfc574",
   "metadata": {},
   "source": [
    "## 5. Standardize Crisis Data\n",
    "\n",
    "Select and reorder crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0969d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing crisis data...\n",
      "\n",
      "‚úì Crisis standardized: 20,855 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uma trag√©dia! For√ßa, RS! RT @JornalOGlobo: Inc√™ndio em boate de #SantaMaria j√° √© o segundo maior...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crisislex</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>2013_Brazil_nightclub_fire</td>\n",
       "      <td>related_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Greece #Fire The fires have claimed 79 lives in Greece and the death toll is expected to climb,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humaid</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>greece_wildfires_2018_train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ch150ch: Abbott (who claims exps for runs, swims &amp;amp; bike rides) changes rules so less peo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crisislex</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>2013_Australia_bushfire</td>\n",
       "      <td>related_informative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  Uma trag√©dia! For√ßa, RS! RT @JornalOGlobo: Inc√™ndio em boate de #SantaMaria j√° √© o segundo maior...   \n",
       "1  #Greece #Fire The fires have claimed 79 lives in Greece and the death toll is expected to climb,...   \n",
       "2  RT @ch150ch: Abbott (who claims exps for runs, swims &amp; bike rides) changes rules so less peo...   \n",
       "\n",
       "   emotion_label  emotion_name source_dataset  crisis_label event_type  \\\n",
       "0            NaN           NaN      crisislex             1   wildfire   \n",
       "1            NaN           NaN         humaid             1   wildfire   \n",
       "2            NaN           NaN      crisislex             1   wildfire   \n",
       "\n",
       "                    event_name      informativeness  \n",
       "0   2013_Brazil_nightclub_fire  related_informative  \n",
       "1  greece_wildfires_2018_train                  NaN  \n",
       "2      2013_Australia_bushfire  related_informative  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing crisis data...\\n\")\n",
    "\n",
    "# Create standardized crisis dataframe\n",
    "df_crisis_std = pd.DataFrame()\n",
    "\n",
    "df_crisis_std['text'] = df_crisis['text']\n",
    "df_crisis_std['emotion_label'] = df_crisis['emotion_label']  # Will be NaN\n",
    "df_crisis_std['emotion_name'] = df_crisis['emotion_name']    # Will be empty\n",
    "df_crisis_std['source_dataset'] = df_crisis['source_dataset']\n",
    "df_crisis_std['crisis_label'] = df_crisis['crisis_label']\n",
    "df_crisis_std['event_type'] = df_crisis['event_type']\n",
    "df_crisis_std['event_name'] = df_crisis['event_name']\n",
    "df_crisis_std['informativeness'] = df_crisis['informativeness']\n",
    "\n",
    "print(f\"‚úì Crisis standardized: {len(df_crisis_std):,} rows\")\n",
    "print(f\"  Columns: {df_crisis_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_crisis_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702522d",
   "metadata": {},
   "source": [
    "## 6. Standardize Non-Crisis Data\n",
    "\n",
    "Select and reorder non-crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd741cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing non-crisis data...\n",
      "\n",
      "‚úì Non-crisis standardized: 17,256 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@YELLOWCLAW at @coachella is happening, life is good! #ymfc #Coachella2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coachella</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>coachella_2015</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@portlandaniel @coachella Ah man, that's a bummer! Caleb and I have been training for it. #coach...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coachella</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>coachella_2015</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@flo_tweet and @MarinasDiamonds lined up for #Coachella2015 _√ô√∑¬ù</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coachella</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>coachella_2015</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                           @YELLOWCLAW at @coachella is happening, life is good! #ymfc #Coachella2015   \n",
       "1  @portlandaniel @coachella Ah man, that's a bummer! Caleb and I have been training for it. #coach...   \n",
       "2                                     @flo_tweet and @MarinasDiamonds lined up for #Coachella2015 _√ô√∑¬ù   \n",
       "\n",
       "   emotion_label  emotion_name source_dataset  crisis_label     event_type  \\\n",
       "0            NaN           NaN      coachella             0  entertainment   \n",
       "1            NaN           NaN      coachella             0  entertainment   \n",
       "2            NaN           NaN      coachella             0  entertainment   \n",
       "\n",
       "       event_name informativeness  \n",
       "0  coachella_2015                  \n",
       "1  coachella_2015                  \n",
       "2  coachella_2015                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing non-crisis data...\\n\")\n",
    "\n",
    "# Create standardized non-crisis dataframe\n",
    "df_non_crisis_std = pd.DataFrame()\n",
    "\n",
    "df_non_crisis_std['text'] = df_non_crisis['text']\n",
    "df_non_crisis_std['emotion_label'] = df_non_crisis['emotion_label']  # Will be NaN\n",
    "df_non_crisis_std['emotion_name'] = df_non_crisis['emotion_name']    # Will be empty\n",
    "df_non_crisis_std['source_dataset'] = df_non_crisis['source_dataset']\n",
    "df_non_crisis_std['crisis_label'] = df_non_crisis['crisis_label']\n",
    "df_non_crisis_std['event_type'] = df_non_crisis['event_type']\n",
    "df_non_crisis_std['event_name'] = df_non_crisis['event_name']\n",
    "\n",
    "# Non-crisis doesn't have informativeness\n",
    "df_non_crisis_std['informativeness'] = ''\n",
    "\n",
    "print(f\"‚úì Non-crisis standardized: {len(df_non_crisis_std):,} rows\")\n",
    "print(f\"  Columns: {df_non_crisis_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_non_crisis_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3fcb7",
   "metadata": {},
   "source": [
    "## 7. Validate Schema Alignment\n",
    "\n",
    "Ensure all three datasets have identical column structure before combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec744ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SCHEMA VALIDATION\n",
      "================================================================================\n",
      "\n",
      "GoEmotions columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "Crisis columns:     ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "Non-crisis columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "\n",
      "‚úÖ All datasets have matching column structure!\n",
      "\n",
      "‚úÖ Columns match master schema!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SCHEMA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check column names\n",
    "goemotions_cols = df_goemotions_std.columns.tolist()\n",
    "crisis_cols = df_crisis_std.columns.tolist()\n",
    "non_crisis_cols = df_non_crisis_std.columns.tolist()\n",
    "\n",
    "print(f\"\\nGoEmotions columns: {goemotions_cols}\")\n",
    "print(f\"Crisis columns:     {crisis_cols}\")\n",
    "print(f\"Non-crisis columns: {non_crisis_cols}\")\n",
    "\n",
    "# Validate all match\n",
    "if goemotions_cols == crisis_cols == non_crisis_cols:\n",
    "    print(\"\\n‚úÖ All datasets have matching column structure!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Column mismatch detected!\")\n",
    "    print(f\"\\nDifferences:\")\n",
    "    if goemotions_cols != crisis_cols:\n",
    "        print(f\"  GoEmotions vs Crisis: {set(goemotions_cols) ^ set(crisis_cols)}\")\n",
    "    if crisis_cols != non_crisis_cols:\n",
    "        print(f\"  Crisis vs Non-crisis: {set(crisis_cols) ^ set(non_crisis_cols)}\")\n",
    "\n",
    "# Check if columns match master schema\n",
    "if goemotions_cols == MASTER_COLUMNS:\n",
    "    print(\"\\n‚úÖ Columns match master schema!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Column order differs from master schema\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add23f7",
   "metadata": {},
   "source": [
    "## 8. Combine All Datasets\n",
    "\n",
    "Concatenate all three standardized datasets into master training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba252696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "\n",
      "Combined master dataset: 52,767 rows\n",
      "Shuffling dataset to randomize row order...\n",
      "\n",
      "‚úÖ Combined and shuffled master dataset created!\n",
      "\n",
      "Total rows: 52,767\n",
      "\n",
      "Breakdown:\n",
      "  GoEmotions:  14,656 (27.8%)\n",
      "  Crisis:      20,855 (39.5%)\n",
      "  Non-crisis:  17,256 (32.7%)\n",
      "\n",
      "Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "\n",
      "Memory usage: 24.86 MB\n",
      "\n",
      "First 10 rows source distribution (showing shuffle worked):\n",
      "['crisislex', 'us_election', 'GoEmotions', 'crisislex', 'humaid', 'crisislex', 'game_of_thrones', 'crisislex', 'GoEmotions', 'GoEmotions']\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining datasets...\\n\")\n",
    "\n",
    "# Concatenate all datasets\n",
    "df_master = pd.concat([\n",
    "    df_goemotions_std,\n",
    "    df_crisis_std,\n",
    "    df_non_crisis_std\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Combined master dataset: {len(df_master):,} rows\")\n",
    "\n",
    "# Shuffle the dataset so rows are randomized (not grouped by source)\n",
    "print(\"Shuffling dataset to randomize row order...\")\n",
    "df_master = df_master.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Combined and shuffled master dataset created!\")\n",
    "print(f\"\\nTotal rows: {len(df_master):,}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  GoEmotions:  {len(df_goemotions_std):,} ({len(df_goemotions_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Crisis:      {len(df_crisis_std):,} ({len(df_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Non-crisis:  {len(df_non_crisis_std):,} ({len(df_non_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nColumns: {df_master.columns.tolist()}\")\n",
    "print(f\"\\nMemory usage: {df_master.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "# Show that rows are now mixed\n",
    "print(f\"\\nFirst 10 rows source distribution (showing shuffle worked):\")\n",
    "print(df_master.head(10)['source_dataset'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d310a",
   "metadata": {},
   "source": [
    "## 9. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b43a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Null counts:\n",
      "text                   4\n",
      "emotion_label      38111\n",
      "emotion_name       38111\n",
      "source_dataset         0\n",
      "crisis_label       14656\n",
      "event_type             0\n",
      "event_name             0\n",
      "informativeness     8335\n",
      "dtype: int64\n",
      "\n",
      "Text validation:\n",
      "  Null texts: 4\n",
      "  Empty texts: 0\n",
      "\n",
      "Emotion label status:\n",
      "  Labeled (GoEmotions):    14,656 (27.8%)\n",
      "  Unlabeled (Crisis+Non):  38,111 (72.2%)\n",
      "\n",
      "Crisis label distribution:\n",
      "  Crisis (1):      20,855\n",
      "  Non-crisis (0):  17,256\n",
      "  Unlabeled (GoE): 14,656\n",
      "\n",
      "Source dataset distribution:\n",
      "source_dataset\n",
      "GoEmotions         14656\n",
      "crisislex          12733\n",
      "humaid              8122\n",
      "us_election         2571\n",
      "game_of_thrones     2571\n",
      "worldcup_2018       2571\n",
      "tokyo_olympics      2571\n",
      "coachella           2571\n",
      "fifa_worldcup       2571\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "print(f\"\\nNull counts:\")\n",
    "print(df_master.isnull().sum())\n",
    "\n",
    "# Check text column\n",
    "null_text = df_master['text'].isna().sum()\n",
    "empty_text = (df_master['text'] == '').sum()\n",
    "print(f\"\\nText validation:\")\n",
    "print(f\"  Null texts: {null_text}\")\n",
    "print(f\"  Empty texts: {empty_text}\")\n",
    "if null_text == 0 and empty_text == 0:\n",
    "    print(f\"  ‚úÖ All rows have text content\")\n",
    "\n",
    "# Check emotion labels\n",
    "labeled_rows = df_master['emotion_label'].notna().sum()\n",
    "unlabeled_rows = df_master['emotion_label'].isna().sum()\n",
    "print(f\"\\nEmotion label status:\")\n",
    "print(f\"  Labeled (GoEmotions):    {labeled_rows:,} ({labeled_rows/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Unlabeled (Crisis+Non):  {unlabeled_rows:,} ({unlabeled_rows/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "# Check crisis labels\n",
    "crisis_rows = (df_master['crisis_label'] == 1).sum()\n",
    "non_crisis_rows = (df_master['crisis_label'] == 0).sum()\n",
    "unlabeled_crisis = df_master['crisis_label'].isna().sum()\n",
    "print(f\"\\nCrisis label distribution:\")\n",
    "print(f\"  Crisis (1):      {crisis_rows:,}\")\n",
    "print(f\"  Non-crisis (0):  {non_crisis_rows:,}\")\n",
    "print(f\"  Unlabeled (GoE): {unlabeled_crisis:,}\")\n",
    "\n",
    "# Check source distribution\n",
    "print(f\"\\nSource dataset distribution:\")\n",
    "print(df_master['source_dataset'].value_counts())\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3e5d6",
   "metadata": {},
   "source": [
    "## 10. Show Sample Data from Each Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09420da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from each source:\n",
      "\n",
      "GoEmotions sample (with emotion labels):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you don‚Äôt appreciate the good times till you go through the bad.. things will get better OP, sta...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>gratitude</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm hoping it's more statistical noise at this point still, but it's definitely worrying.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Well I can feel it was mentally hard bringing myself to go full [NAME] and shit talking prior to...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "2  you don‚Äôt appreciate the good times till you go through the bad.. things will get better OP, sta...   \n",
       "8            I'm hoping it's more statistical noise at this point still, but it's definitely worrying.   \n",
       "9  Well I can feel it was mentally hard bringing myself to go full [NAME] and shit talking prior to...   \n",
       "\n",
       "   emotion_label emotion_name source_dataset  \n",
       "2           11.0    gratitude     GoEmotions  \n",
       "8            4.0      anxiety     GoEmotions  \n",
       "9            4.0      anxiety     GoEmotions  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There's a bomb explosion again in Texas. From a fertilizer plant. 0_0 #PrayForTexas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013_West_Texas_explosion</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#rescueph Babyshambles follows me personally. Omg. Will be this. I can not believe this, my dear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012_Philipinnes_floods</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @604Now: 200 #BC firefighters head over to #FortMcMurray to assist in wildfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canada_wildfires_2016_test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                  There's a bomb explosion again in Texas. From a fertilizer plant. 0_0 #PrayForTexas   \n",
       "3  #rescueph Babyshambles follows me personally. Omg. Will be this. I can not believe this, my dear...   \n",
       "4                    RT @604Now: 200 #BC firefighters head over to #FortMcMurray to assist in wildfire   \n",
       "\n",
       "   emotion_label emotion_name                  event_name  crisis_label  \n",
       "0            NaN          NaN   2013_West_Texas_explosion           1.0  \n",
       "3            NaN          NaN     2012_Philipinnes_floods           1.0  \n",
       "4            NaN          NaN  canada_wildfires_2016_test           1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@trollBigotry @KamalaHarris I think you mean 47 years of #JoeBiden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>us_election_2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I am so sick of advertisements for that Game Of Thrones mobile game.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got_season8_2019</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@burnt_rain @MikeFromYEG Also one of the main characters on Game of Thrones.&nbsp;&nbsp;He's played by Qui...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got_season8_2019</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   text  \\\n",
       "1                                    @trollBigotry @KamalaHarris I think you mean 47 years of #JoeBiden   \n",
       "6                                  I am so sick of advertisements for that Game Of Thrones mobile game.   \n",
       "10  @burnt_rain @MikeFromYEG Also one of the main characters on Game of Thrones.  He's played by Qui...   \n",
       "\n",
       "    emotion_label emotion_name        event_name  crisis_label  \n",
       "1             NaN          NaN  us_election_2020           0.0  \n",
       "6             NaN          NaN  got_season8_2019           0.0  \n",
       "10            NaN          NaN  got_season8_2019           0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample rows from each source:\\n\")\n",
    "\n",
    "print(\"GoEmotions sample (with emotion labels):\")\n",
    "display(df_master[df_master['source_dataset'] == 'GoEmotions'][['text', 'emotion_label', 'emotion_name', 'source_dataset']].head(3))\n",
    "\n",
    "print(\"\\nCrisis sample (emotion labels = NULL):\")\n",
    "crisis_sample = df_master[df_master['crisis_label'] == 1][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(crisis_sample)\n",
    "\n",
    "print(\"\\nNon-crisis sample (emotion labels = NULL):\")\n",
    "non_crisis_sample = df_master[df_master['crisis_label'] == 0][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(non_crisis_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e489f01",
   "metadata": {},
   "source": [
    "## 11. Save Master Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0010022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving master training dataset to master_training_data/master_training_data_v5.csv...\n",
      "\n",
      "================================================================================\n",
      "MASTER DATASET SAVED\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Saved to: master_training_data/master_training_data_v5.csv\n",
      "\n",
      "File size: 8.04 MB\n",
      "Total rows: 52,767\n",
      "Total columns: 8\n",
      "\n",
      "Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'informativeness']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save to master_training_data folder\n",
    "output_path = 'master_training_data/master_training_data_v5.csv'\n",
    "\n",
    "print(f\"Saving master training dataset to {output_path}...\\n\")\n",
    "\n",
    "df_master.to_csv(output_path, index=False)\n",
    "\n",
    "file_size = Path(output_path).stat().st_size / (1024**2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MASTER DATASET SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Saved to: {output_path}\")\n",
    "print(f\"\\nFile size: {file_size:.2f} MB\")\n",
    "print(f\"Total rows: {len(df_master):,}\")\n",
    "print(f\"Total columns: {len(df_master.columns)}\")\n",
    "print(f\"\\nColumns: {df_master.columns.tolist()}\")\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689328d9",
   "metadata": {},
   "source": [
    "## 12. Create Smaller Sample File for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce99657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created sample file: master_training_data/master_training_sample_v5.csv\n",
      "   Rows: 10,000\n",
      "   Size: 1.50 MB\n"
     ]
    }
   ],
   "source": [
    "# Create 10K sample for quick testing\n",
    "sample_size = min(10000, len(df_master))\n",
    "df_sample = df_master.sample(n=sample_size, random_state=42)\n",
    "\n",
    "sample_path = 'master_training_data/master_training_sample_v5.csv'\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Created sample file: {sample_path}\")\n",
    "print(f\"   Rows: {len(df_sample):,}\")\n",
    "print(f\"   Size: {Path(sample_path).stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ee71",
   "metadata": {},
   "source": [
    "## 13. Final Summary & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67521c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Composition (Stratified 60K):\n",
      "   Total rows:          52,767\n",
      "   Crisis:              20,855 (balanced by event_type)\n",
      "   Non-crisis:          17,256 (balanced by source)\n",
      "   GoEmotions:          14,656 (balanced by emotion)\n",
      "\n",
      "üìÅ Files Created:\n",
      "   Main:   master_training_data/master_training_data_v5.csv (8.04 MB)\n",
      "   Sample: master_training_data/master_training_sample_v5.csv\n",
      "\n",
      "üè∑Ô∏è Label Status (to be filled by LLM in notebook 06):\n",
      "   emotion_label:  14,656 labeled, 38,111 need LLM\n",
      "   crisis_label:   38,111 labeled, 14,656 need LLM\n",
      "\n",
      "üîß Schema:\n",
      "   Columns: 8\n",
      "      1. text\n",
      "      2. emotion_label\n",
      "      3. emotion_name\n",
      "      4. source_dataset\n",
      "      5. crisis_label\n",
      "      6. event_type\n",
      "      7. event_name\n",
      "      8. informativeness\n",
      "\n",
      "üìã Next Steps:\n",
      "   1. Run notebook 06 to fill missing labels using Gemini API\n",
      "   2. Train multi-task BERT on labeled 60K dataset\n",
      "   3. Apply trained BERT to ORIGINAL FULL datasets\n",
      "   4. Create episodes & hourly aggregations for RL agent\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PHASE 4 COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Composition (Stratified 60K):\")\n",
    "print(f\"   Total rows:          {len(df_master):,}\")\n",
    "print(f\"   Crisis:              {len(df_crisis_std):,} (balanced by event_type)\")\n",
    "print(f\"   Non-crisis:          {len(df_non_crisis_std):,} (balanced by source)\")\n",
    "print(f\"   GoEmotions:          {len(df_goemotions_std):,} (balanced by emotion)\")\n",
    "\n",
    "print(f\"\\nüìÅ Files Created:\")\n",
    "print(f\"   Main:   master_training_data/master_training_data_v5.csv ({file_size:.2f} MB)\")\n",
    "print(f\"   Sample: master_training_data/master_training_sample_v5.csv\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Label Status (to be filled by LLM in notebook 06):\")\n",
    "print(f\"   emotion_label:  {df_master['emotion_label'].notna().sum():,} labeled, {df_master['emotion_label'].isna().sum():,} need LLM\")\n",
    "print(f\"   crisis_label:   {df_master['crisis_label'].notna().sum():,} labeled, {df_master['crisis_label'].isna().sum():,} need LLM\")\n",
    "\n",
    "print(f\"\\nüîß Schema:\")\n",
    "print(f\"   Columns: {len(df_master.columns)}\")\n",
    "for i, col in enumerate(df_master.columns, 1):\n",
    "    print(f\"      {i}. {col}\")\n",
    "\n",
    "print(f\"\\nüìã Next Steps:\")\n",
    "print(f\"   1. Run notebook 06 to fill missing labels using Gemini API\")\n",
    "print(f\"   2. Train multi-task BERT on labeled 60K dataset\")\n",
    "print(f\"   3. Apply trained BERT to ORIGINAL FULL datasets\")\n",
    "print(f\"   4. Create episodes & hourly aggregations for RL agent\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PHASE 4 COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
