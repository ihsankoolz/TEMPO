{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7402f641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 10000\n",
      "WRITE_FULL_MASTER: False\n"
     ]
    }
   ],
   "source": [
    "# Parameters for reproducible sample generation\n",
    "# Set WRITE_FULL_MASTER=True to write the full master file (v4). Default is False to avoid large writes.\n",
    "SAMPLE_SIZE_10K = 10000\n",
    "WRITE_FULL_MASTER = False\n",
    "SAMPLE_FILE_10KV3 = \"master_training_data/master_training_sample_10kv3.csv\"\n",
    "MASTER_FILE_V4 = \"master_training_data/master_training_data_v4.csv\"\n",
    "\n",
    "print(f\"Sample size: {SAMPLE_SIZE_10K}\")\n",
    "print(f\"WRITE_FULL_MASTER: {WRITE_FULL_MASTER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824755c",
   "metadata": {},
   "source": [
    "# Phase 4: Create Master Training Dataset\n",
    "## Combine GoEmotions + Crisis + Non-Crisis Data\n",
    "\n",
    "This notebook creates a **reduced, balanced dataset for training multi-task BERT**.\n",
    "\n",
    "### What this dataset is for:\n",
    "- **Train BERT** on emotion classification (using GoEmotions labels)\n",
    "- **Train BERT** on crisis detection (using crisis_label)\n",
    "- Smaller dataset (~217K rows) for efficient training\n",
    "\n",
    "### What happens after BERT is trained:\n",
    "1. Apply trained BERT to **ORIGINAL FULL datasets** (1.5M+ non-crisis, 67K crisis)\n",
    "2. Extract emotion features for ALL tweets\n",
    "3. Use these features to create episodes & hourly aggregations for RL agent\n",
    "\n",
    "### Data Sources:\n",
    "- **GoEmotions**: 54K Reddit comments with labeled emotions (for BERT training)\n",
    "- **Crisis**: 67K crisis tweets (all kept)\n",
    "- **Non-Crisis**: ~96K sampled non-crisis tweets (reduced from 1.5M for balanced training)\n",
    "\n",
    "### Sampling Strategy:\n",
    "- Non-crisis data is randomly sampled with sports emphasis\n",
    "- Dataset is shuffled so rows are randomized (not grouped by source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9085155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6913015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running create_master_training_file.py\n",
      "================================================================================\n",
      "CREATING MASTER TRAINING FILE FOR MULTI-TASK BERT\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: PROCESSING INDIVIDUAL DATASETS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROCESSING GOEMOTIONS\n",
      "================================================================================\n",
      "Loading: ./goemotion_data/goemotions.csv\n",
      "Loaded: 54,263 rows\n",
      "Processing emotion labels...\n",
      "\n",
      "Emotion Distribution (13 emotions):\n",
      "   emotion_fear             : 764 tweets\n",
      "   emotion_sadness          : 1,625 tweets\n",
      "   emotion_anger            : 1,960 tweets\n",
      "   emotion_nervousness      : 208 tweets\n",
      "   emotion_disgust          : 1,013 tweets\n",
      "   emotion_surprise         : 1,330 tweets\n",
      "   emotion_confusion        : 1,673 tweets\n",
      "   emotion_caring           : 1,375 tweets\n",
      "   emotion_grief            : 96 tweets\n",
      "   emotion_disappointment   : 1,583 tweets\n",
      "   emotion_joy              : 1,785 tweets\n",
      "   emotion_relief           : 182 tweets\n",
      "   emotion_neutral          : 17,772 tweets\n",
      "\n",
      "GoEmotions Processed: 54,263 rows\n",
      "\n",
      "================================================================================\n",
      "PROCESSING CRISIS DATASETS\n",
      "================================================================================\n",
      "Loading: ./standardized_data/crisis_combined_dates_only.csv\n",
      "Loaded: 66,748 rows\n",
      "\n",
      "Crisis Data Processed: 66,748 rows\n",
      "\n",
      "================================================================================\n",
      "PROCESSING NON-CRISIS DATASETS\n",
      "================================================================================\n",
      "Loading: ./standardized_data/non_crisis_combined.csv\n",
      "Loaded: 1,533,696 rows\n",
      "\n",
      "Non-Crisis Data Processed: 1,533,696 rows\n",
      "\n",
      "================================================================================\n",
      "STEP 2: COMBINING INTO MASTER FILE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMBINING ALL DATASETS\n",
      "================================================================================\n",
      "\n",
      "Combining 3 datasets: GoEmotions, Crisis, Non-Crisis\n",
      "Initial combine: 1,654,707 total rows\n",
      "\n",
      "SHUFFLING DATA (critical for multi-task training)...\n",
      "Data shuffled!\n",
      "\n",
      "================================================================================\n",
      "MASTER TRAINING FILE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "TOTAL ROWS: 1,654,707\n",
      "\n",
      "BY SOURCE DATASET:\n",
      "source_dataset\n",
      "game_of_thrones    760614\n",
      "worldcup_2018      458533\n",
      "tokyo_olympics     159432\n",
      "us_election         99948\n",
      "goemotions          54263\n",
      "fifa_worldcup       49493\n",
      "humaid              43409\n",
      "crisislex           23339\n",
      "coachella            3846\n",
      "music_concerts       1830\n",
      "\n",
      "BY EVENT TYPE:\n",
      "event_type\n",
      "entertainment    766290\n",
      "sports           667458\n",
      "politics          99948\n",
      "None              54263\n",
      "hurricane         33422\n",
      "earthquake        11429\n",
      "flood              7498\n",
      "wildfire           7151\n",
      "accident           4248\n",
      "bombing            2000\n",
      "haze               1000\n",
      "\n",
      "LABEL AVAILABILITY:\n",
      "   Emotion labels: 54,263 rows\n",
      "   Event type labels: 1,600,444 rows\n",
      "   Informativeness labels: 22,932 rows\n",
      "\n",
      "Saving sample (10000) for review...\n",
      "SAMPLE SAVED: ./master_training_data/master_training_sample_10kv3.csv\n",
      "\n",
      "Skipping full master write (MASTER_FILE_V4). To enable, set WRITE_FULL_MASTER = True in the script.\n",
      "\n",
      "================================================================================\n",
      "MASTER TRAINING FILE CREATED!\n",
      "================================================================================\n",
      "\n",
      "Files created:\n",
      "   - master_training_data.csv (1,654,707 rows)\n",
      "   - master_training_sample_1000.csv (for preview)\n",
      "\n",
      "Datasets included:\n",
      "   - GoEmotions (emotions)\n",
      "   - Crisis datasets (event types + informativeness)\n",
      "   - Non-crisis datasets (event types)\n",
      "\n",
      "READY FOR MULTI-TASK BERT TRAINING!\n",
      "        \n",
      "Return code: 0\n",
      "\n",
      "Files produced (if any):\n",
      "master_training_data/master_training_sample_10kv3.csv -> exists\n",
      "master_training_data/master_training_data_v4.csv -> MISSING\n"
     ]
    }
   ],
   "source": [
    "# Run the master creation script to produce the 10k sample (non-destructive)\n",
    "# This will respect the WRITE_FULL_MASTER flag defined above.\n",
    "import subprocess, sys, os\n",
    "\n",
    "print('Running create_master_training_file.py')\n",
    "ret = subprocess.run([sys.executable, 'scripts/phase4_combine/create_master_training_file.py'], check=False)\n",
    "print('Return code:', ret.returncode)\n",
    "\n",
    "# Confirm files\n",
    "print('\\nFiles produced (if any):')\n",
    "for p in [SAMPLE_FILE_10KV3, SAMPLE_FILE_10KV3_IMPUTED, MASTER_FILE_V4]:\n",
    "    print(p, '->', 'exists' if os.path.exists(p) else 'MISSING')\n",
    "\n",
    "# Quick verification: if an imputed sample exists, show counts\n",
    "imputed_path = SAMPLE_FILE_10KV3_IMPUTED if os.path.exists(SAMPLE_FILE_10KV3_IMPUTED) else (SAMPLE_FILE_10KV3 if os.path.exists(SAMPLE_FILE_10KV3) else None)\n",
    "if imputed_path is not None:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(imputed_path)\n",
    "    print('\\nLoaded sample:', imputed_path)\n",
    "    print('Rows:', len(df))\n",
    "    print('created_at NA:', pd.to_datetime(df['created_at'], errors='coerce').isna().sum())\n",
    "    if 'created_at_imputed' in df.columns:\n",
    "        print('created_at_imputed True:', int(df['created_at_imputed'].sum()))\n",
    "        print('imputation methods top:', df['created_at_imputed_method'].value_counts().head().to_string())\n",
    "    else:\n",
    "        print('No imputation flags present in sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378adf6",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f45d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "1. GoEmotions (with 13 emotions)...\n",
      "   ✓ Loaded 54,263 rows\n",
      "   Columns: ['text', 'emotion_label', 'emotion_name', 'id', 'labels']\n",
      "\n",
      "2. Crisis data (with emotion columns)...\n",
      "   ✓ Loaded 66,748 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'informativeness', 'created_at_imputed', 'created_at_imputed_method', 'emotion_label', 'emotion_name']\n",
      "\n",
      "3. Non-crisis data (with emotion columns)...\n",
      "   ✓ Loaded 1,533,696 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'emotion_label', 'emotion_name']\n",
      "\n",
      "================================================================================\n",
      "Total rows to combine: 1,654,707\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "# Load GoEmotions with 13 emotions\n",
    "print(\"1. GoEmotions (with 13 emotions)...\")\n",
    "df_goemotions = pd.read_csv('goemotion_data/goemotions_with_13_emotions.csv')\n",
    "print(f\"   ✓ Loaded {len(df_goemotions):,} rows\")\n",
    "print(f\"   Columns: {df_goemotions.columns.tolist()}\")\n",
    "\n",
    "# Load crisis data with emotion columns\n",
    "print(\"\\n2. Crisis data (with emotion columns)...\")\n",
    "df_crisis = pd.read_csv('standardized_data/crisis_combined_with_emotions.csv')\n",
    "print(f\"   ✓ Loaded {len(df_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_crisis.columns.tolist()}\")\n",
    "\n",
    "# Load non-crisis data with emotion columns\n",
    "print(\"\\n3. Non-crisis data (with emotion columns)...\")\n",
    "df_non_crisis = pd.read_csv('standardized_data/non_crisis_combined_with_emotions.csv')\n",
    "print(f\"   ✓ Loaded {len(df_non_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_non_crisis.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total rows to combine: {len(df_goemotions) + len(df_crisis) + len(df_non_crisis):,}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ogmggp72bb",
   "metadata": {},
   "source": [
    "## 1.5 Sample Non-Crisis Data (Reduce Dataset Size)\n",
    "\n",
    "Downsample non-crisis data from 1.5M to ~100K rows:\n",
    "- Keep all GoEmotions (54K) - needed for training\n",
    "- Keep all Crisis (67K) - core data for crisis detection\n",
    "- Sample non-crisis to ~100K with sports emphasis\n",
    "\n",
    "**Sampling Distribution:**\n",
    "| Source | Type | Sample Size |\n",
    "|--------|------|-------------|\n",
    "| worldcup_2018 | Sports | 20,000 |\n",
    "| tokyo_olympics | Sports | 20,000 |\n",
    "| fifa_worldcup | Sports | 20,000 |\n",
    "| game_of_thrones | Entertainment | 20,000 |\n",
    "| us_election | Politics | 10,000 |\n",
    "| coachella | Entertainment | All (~3,846) |\n",
    "| music_concerts | Entertainment | All (~1,830) |\n",
    "\n",
    "**Rationale:** Sports-heavy distribution helps model learn to distinguish sports excitement from crisis fear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dforzd1aoro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling non-crisis data...\n",
      "\n",
      "Original non-crisis distribution:\n",
      "source_dataset\n",
      "game_of_thrones    760614\n",
      "worldcup_2018      458533\n",
      "tokyo_olympics     159432\n",
      "us_election         99948\n",
      "fifa_worldcup       49493\n",
      "coachella            3846\n",
      "music_concerts       1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total before sampling: 1,533,696\n",
      "  worldcup_2018: 20,000 (sampled from 458,533)\n",
      "  tokyo_olympics: 20,000 (sampled from 159,432)\n",
      "  fifa_worldcup: 20,000 (sampled from 49,493)\n",
      "  game_of_thrones: 20,000 (sampled from 760,614)\n",
      "  us_election: 10,000 (sampled from 99,948)\n",
      "  coachella: 3,846 (kept all)\n",
      "  music_concerts: 1,830 (kept all)\n",
      "\n",
      "============================================================\n",
      "Non-crisis sampling complete!\n",
      "  Before: 1,533,696 rows\n",
      "  After:  95,676 rows\n",
      "  Reduction: 93.8%\n",
      "============================================================\n",
      "\n",
      "New non-crisis distribution:\n",
      "source_dataset\n",
      "worldcup_2018      20000\n",
      "tokyo_olympics     20000\n",
      "fifa_worldcup      20000\n",
      "game_of_thrones    20000\n",
      "us_election        10000\n",
      "coachella           3846\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampling non-crisis data...\\n\")\n",
    "\n",
    "# Define sampling configuration\n",
    "SAMPLE_CONFIG = {\n",
    "    'worldcup_2018': 20000,      # Sports\n",
    "    'tokyo_olympics': 20000,     # Sports\n",
    "    'fifa_worldcup': 20000,      # Sports\n",
    "    'game_of_thrones': 20000,    # Entertainment\n",
    "    'us_election': 10000,        # Politics\n",
    "    'coachella': None,           # Keep all (small dataset)\n",
    "    'music_concerts': None,      # Keep all (small dataset)\n",
    "}\n",
    "\n",
    "print(\"Original non-crisis distribution:\")\n",
    "print(df_non_crisis['source_dataset'].value_counts())\n",
    "print(f\"\\nTotal before sampling: {len(df_non_crisis):,}\")\n",
    "\n",
    "# Sample each source\n",
    "sampled_dfs = []\n",
    "for source, sample_size in SAMPLE_CONFIG.items():\n",
    "    source_df = df_non_crisis[df_non_crisis['source_dataset'] == source]\n",
    "    \n",
    "    if sample_size is None or len(source_df) <= sample_size:\n",
    "        # Keep all rows for small datasets\n",
    "        sampled_dfs.append(source_df)\n",
    "        print(f\"  {source}: {len(source_df):,} (kept all)\")\n",
    "    else:\n",
    "        # Random sample for large datasets\n",
    "        sampled = source_df.sample(n=sample_size, random_state=42)\n",
    "        sampled_dfs.append(sampled)\n",
    "        print(f\"  {source}: {sample_size:,} (sampled from {len(source_df):,})\")\n",
    "\n",
    "# Combine sampled data\n",
    "df_non_crisis_sampled = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Non-crisis sampling complete!\")\n",
    "print(f\"  Before: {len(df_non_crisis):,} rows\")\n",
    "print(f\"  After:  {len(df_non_crisis_sampled):,} rows\")\n",
    "print(f\"  Reduction: {(1 - len(df_non_crisis_sampled)/len(df_non_crisis))*100:.1f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Replace original with sampled version\n",
    "df_non_crisis = df_non_crisis_sampled\n",
    "\n",
    "print(\"\\nNew non-crisis distribution:\")\n",
    "print(df_non_crisis['source_dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bdfd9",
   "metadata": {},
   "source": [
    "## 2. Check Current Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b68ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current column schemas:\n",
      "\n",
      "GoEmotions columns:\n",
      "  - text: str\n",
      "  - emotion_label: int64\n",
      "  - emotion_name: str\n",
      "  - id: str\n",
      "  - labels: str\n",
      "\n",
      "Crisis columns:\n",
      "  - text: str\n",
      "  - created_at: str\n",
      "  - event_name: str\n",
      "  - event_type: str\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: str\n",
      "  - informativeness: str\n",
      "  - created_at_imputed: bool\n",
      "  - created_at_imputed_method: float64\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n",
      "\n",
      "Non-crisis columns:\n",
      "  - text: str\n",
      "  - created_at: str\n",
      "  - event_name: str\n",
      "  - event_type: str\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: str\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Current column schemas:\\n\")\n",
    "\n",
    "print(\"GoEmotions columns:\")\n",
    "for col in df_goemotions.columns:\n",
    "    print(f\"  - {col}: {df_goemotions[col].dtype}\")\n",
    "\n",
    "print(\"\\nCrisis columns:\")\n",
    "for col in df_crisis.columns:\n",
    "    print(f\"  - {col}: {df_crisis[col].dtype}\")\n",
    "\n",
    "print(\"\\nNon-crisis columns:\")\n",
    "for col in df_non_crisis.columns:\n",
    "    print(f\"  - {col}: {df_non_crisis[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d6cb2",
   "metadata": {},
   "source": [
    "## 3. Define Master Schema\n",
    "\n",
    "Create unified column structure for all datasets:\n",
    "- **text**: Tweet/comment text\n",
    "- **emotion_label**: Numeric emotion (1-13, NULL for unlabeled)\n",
    "- **emotion_name**: Text emotion name (NULL for unlabeled)\n",
    "- **source_dataset**: Origin of data (GoEmotions, HumAID, CrisisLex, etc.)\n",
    "- **crisis_label**: Binary (1=crisis, 0=non-crisis, NULL for GoEmotions)\n",
    "- **event_type**: General category (hurricane, sports, etc., NULL for GoEmotions)\n",
    "- **event_name**: Specific event (hurricane_harvey_2017, etc., NULL for GoEmotions)\n",
    "- **created_at**: Timestamp (NULL for GoEmotions)\n",
    "- **informativeness**: CrisisLex informativeness label (NULL for others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "403345da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master schema columns:\n",
      "  1. text\n",
      "  2. emotion_label\n",
      "  3. emotion_name\n",
      "  4. source_dataset\n",
      "  5. crisis_label\n",
      "  6. event_type\n",
      "  7. event_name\n",
      "  8. created_at\n",
      "  9. informativeness\n"
     ]
    }
   ],
   "source": [
    "# Define master column set\n",
    "MASTER_COLUMNS = [\n",
    "    'text',\n",
    "    'emotion_label',\n",
    "    'emotion_name',\n",
    "    'source_dataset',\n",
    "    'crisis_label',\n",
    "    'event_type',\n",
    "    'event_name',\n",
    "    'created_at',\n",
    "    'informativeness'\n",
    "]\n",
    "\n",
    "print(\"Master schema columns:\")\n",
    "for i, col in enumerate(MASTER_COLUMNS, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec4a07",
   "metadata": {},
   "source": [
    "## 4. Standardize GoEmotions Data\n",
    "\n",
    "Add missing columns to GoEmotions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e49b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing GoEmotions data...\n",
      "\n",
      "✓ GoEmotions standardized: 54,263 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to cook myself.</td>\n",
       "      <td>13</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will think hes having a laugh screwing with people instead ...</td>\n",
       "      <td>13</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                                          My favourite food is anything I didn't have to cook myself.   \n",
       "1  Now if he does off himself, everyone will think hes having a laugh screwing with people instead ...   \n",
       "2                                                                       WHY THE FUCK IS BAYLESS ISOING   \n",
       "\n",
       "   emotion_label emotion_name source_dataset  crisis_label event_type  \\\n",
       "0             13      neutral     GoEmotions           NaN              \n",
       "1             13      neutral     GoEmotions           NaN              \n",
       "2              2        anger     GoEmotions           NaN              \n",
       "\n",
       "  event_name created_at informativeness  \n",
       "0                                        \n",
       "1                                        \n",
       "2                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "from utils.impute_missing_dates import impute_missing_dates\n",
    "\n",
    "print(\"Standardizing GoEmotions data...\\n\")\n",
    "\n",
    "# Create standardized GoEmotions dataframe\n",
    "df_goemotions_std = pd.DataFrame()\n",
    "\n",
    "# Keep existing columns\n",
    "df_goemotions_std['text'] = df_goemotions['text']\n",
    "df_goemotions_std['emotion_label'] = df_goemotions['emotion_label']\n",
    "df_goemotions_std['emotion_name'] = df_goemotions['emotion_name']\n",
    "\n",
    "# Add source\n",
    "df_goemotions_std['source_dataset'] = 'GoEmotions'\n",
    "\n",
    "# Add NULL columns (GoEmotions is not crisis-related)\n",
    "df_goemotions_std['crisis_label'] = np.nan\n",
    "df_goemotions_std['event_type'] = ''\n",
    "df_goemotions_std['event_name'] = ''\n",
    "\n",
    "# Use NaT for missing dates (proper datetime null value)\n",
    "df_goemotions_std['created_at'] = pd.NaT\n",
    "\n",
    "df_goemotions_std['informativeness'] = ''\n",
    "\n",
    "# Apply date imputation using sample_pool method\n",
    "print(\"Imputing missing dates for GoEmotions using sample_pool method...\")\n",
    "df_goemotions_std = impute_missing_dates(\n",
    "    df_goemotions_std,\n",
    "    method='sample_pool',\n",
    "    reference_col='source_dataset',\n",
    "    jitter_hours=6\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ GoEmotions standardized: {len(df_goemotions_std):,} rows\")\n",
    "print(f\"  Columns: {df_goemotions_std.columns.tolist()}\")\n",
    "if 'created_at_imputed' in df_goemotions_std.columns:\n",
    "    print(f\"  Dates imputed: {df_goemotions_std['created_at_imputed'].sum():,}\")\n",
    "    print(f\"  Imputation method: {df_goemotions_std['created_at_imputed_method'].value_counts().to_dict()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_goemotions_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cfc574",
   "metadata": {},
   "source": [
    "## 5. Standardize Crisis Data\n",
    "\n",
    "Select and reorder crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0969d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing crisis data...\n",
      "\n",
      "✓ Crisis standardized: 66,748 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel a little uneasy about the idea of work tomorrow when the aftershocks are still so strong....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humaid</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>kaikoura_earthquake_2016_train</td>\n",
       "      <td>2016-11-14 07:27:53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#eqnz Interislander ferry docking aborted after huge 7.5 magnitude quake, sailings on hold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humaid</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>kaikoura_earthquake_2016_train</td>\n",
       "      <td>2016-11-13 21:27:49</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Much of New Zealand felt the earthquake after midnight; waking to discover how much damage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humaid</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>kaikoura_earthquake_2016_train</td>\n",
       "      <td>2016-11-13 18:25:16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  I feel a little uneasy about the idea of work tomorrow when the aftershocks are still so strong....   \n",
       "1           #eqnz Interislander ferry docking aborted after huge 7.5 magnitude quake, sailings on hold   \n",
       "2           Much of New Zealand felt the earthquake after midnight; waking to discover how much damage   \n",
       "\n",
       "   emotion_label  emotion_name source_dataset  crisis_label  event_type  \\\n",
       "0            NaN           NaN         humaid             1  earthquake   \n",
       "1            NaN           NaN         humaid             1  earthquake   \n",
       "2            NaN           NaN         humaid             1  earthquake   \n",
       "\n",
       "                       event_name           created_at informativeness  \n",
       "0  kaikoura_earthquake_2016_train  2016-11-14 07:27:53             NaN  \n",
       "1  kaikoura_earthquake_2016_train  2016-11-13 21:27:49             NaN  \n",
       "2  kaikoura_earthquake_2016_train  2016-11-13 18:25:16             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing crisis data...\\n\")\n",
    "\n",
    "# Create standardized crisis dataframe\n",
    "df_crisis_std = pd.DataFrame()\n",
    "\n",
    "df_crisis_std['text'] = df_crisis['text']\n",
    "df_crisis_std['emotion_label'] = df_crisis['emotion_label']  # Will be NaN\n",
    "df_crisis_std['emotion_name'] = df_crisis['emotion_name']    # Will be empty\n",
    "df_crisis_std['source_dataset'] = df_crisis['source_dataset']\n",
    "df_crisis_std['crisis_label'] = df_crisis['crisis_label']\n",
    "df_crisis_std['event_type'] = df_crisis['event_type']\n",
    "df_crisis_std['event_name'] = df_crisis['event_name']\n",
    "df_crisis_std['created_at'] = df_crisis['created_at']\n",
    "df_crisis_std['informativeness'] = df_crisis['informativeness']\n",
    "\n",
    "print(f\"✓ Crisis standardized: {len(df_crisis_std):,} rows\")\n",
    "print(f\"  Columns: {df_crisis_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_crisis_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702522d",
   "metadata": {},
   "source": [
    "## 6. Standardize Non-Crisis Data\n",
    "\n",
    "Select and reorder non-crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfd741cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing non-crisis data...\n",
      "\n",
      "✓ Non-crisis standardized: 95,676 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thibuat Courtois Winner Golden Glove Fifa World Cup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worldcup_2018</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>2018-07-15 17:38:07</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paolo Dybala scored more goals for Juventus last season than he played minutes at the Goals For ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worldcup_2018</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>2018-06-30 16:55:14</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France have won the FIFA in Moscow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worldcup_2018</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>2018-07-15 17:56:43</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                                                  Thibuat Courtois Winner Golden Glove Fifa World Cup   \n",
       "1  Paolo Dybala scored more goals for Juventus last season than he played minutes at the Goals For ...   \n",
       "2                                                                   France have won the FIFA in Moscow   \n",
       "\n",
       "   emotion_label  emotion_name source_dataset  crisis_label event_type  \\\n",
       "0            NaN           NaN  worldcup_2018             0     sports   \n",
       "1            NaN           NaN  worldcup_2018             0     sports   \n",
       "2            NaN           NaN  worldcup_2018             0     sports   \n",
       "\n",
       "           event_name           created_at informativeness  \n",
       "0  fifa_worldcup_2018  2018-07-15 17:38:07                  \n",
       "1  fifa_worldcup_2018  2018-06-30 16:55:14                  \n",
       "2  fifa_worldcup_2018  2018-07-15 17:56:43                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing non-crisis data...\\n\")\n",
    "\n",
    "# Create standardized non-crisis dataframe\n",
    "df_non_crisis_std = pd.DataFrame()\n",
    "\n",
    "df_non_crisis_std['text'] = df_non_crisis['text']\n",
    "df_non_crisis_std['emotion_label'] = df_non_crisis['emotion_label']  # Will be NaN\n",
    "df_non_crisis_std['emotion_name'] = df_non_crisis['emotion_name']    # Will be empty\n",
    "df_non_crisis_std['source_dataset'] = df_non_crisis['source_dataset']\n",
    "df_non_crisis_std['crisis_label'] = df_non_crisis['crisis_label']\n",
    "df_non_crisis_std['event_type'] = df_non_crisis['event_type']\n",
    "df_non_crisis_std['event_name'] = df_non_crisis['event_name']\n",
    "df_non_crisis_std['created_at'] = df_non_crisis['created_at']\n",
    "\n",
    "# Non-crisis doesn't have informativeness\n",
    "df_non_crisis_std['informativeness'] = ''\n",
    "\n",
    "print(f\"✓ Non-crisis standardized: {len(df_non_crisis_std):,} rows\")\n",
    "print(f\"  Columns: {df_non_crisis_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_non_crisis_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3fcb7",
   "metadata": {},
   "source": [
    "## 7. Validate Schema Alignment\n",
    "\n",
    "Ensure all three datasets have identical column structure before combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec744ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SCHEMA VALIDATION\n",
      "================================================================================\n",
      "\n",
      "GoEmotions columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "Crisis columns:     ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "Non-crisis columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "✅ All datasets have matching column structure!\n",
      "\n",
      "✅ Columns match master schema!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SCHEMA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check column names\n",
    "goemotions_cols = df_goemotions_std.columns.tolist()\n",
    "crisis_cols = df_crisis_std.columns.tolist()\n",
    "non_crisis_cols = df_non_crisis_std.columns.tolist()\n",
    "\n",
    "print(f\"\\nGoEmotions columns: {goemotions_cols}\")\n",
    "print(f\"Crisis columns:     {crisis_cols}\")\n",
    "print(f\"Non-crisis columns: {non_crisis_cols}\")\n",
    "\n",
    "# Validate all match\n",
    "if goemotions_cols == crisis_cols == non_crisis_cols:\n",
    "    print(\"\\n✅ All datasets have matching column structure!\")\n",
    "else:\n",
    "    print(\"\\n❌ Column mismatch detected!\")\n",
    "    print(f\"\\nDifferences:\")\n",
    "    if goemotions_cols != crisis_cols:\n",
    "        print(f\"  GoEmotions vs Crisis: {set(goemotions_cols) ^ set(crisis_cols)}\")\n",
    "    if crisis_cols != non_crisis_cols:\n",
    "        print(f\"  Crisis vs Non-crisis: {set(crisis_cols) ^ set(non_crisis_cols)}\")\n",
    "\n",
    "# Check if columns match master schema\n",
    "if goemotions_cols == MASTER_COLUMNS:\n",
    "    print(\"\\n✅ Columns match master schema!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Column order differs from master schema\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add23f7",
   "metadata": {},
   "source": [
    "## 8. Combine All Datasets\n",
    "\n",
    "Concatenate all three standardized datasets into master training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba252696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "\n",
      "Combined master dataset: 216,687 rows\n",
      "Shuffling dataset to randomize row order...\n",
      "\n",
      "✅ Combined and shuffled master dataset created!\n",
      "\n",
      "Total rows: 216,687\n",
      "\n",
      "Breakdown:\n",
      "  GoEmotions:  54,263 (25.0%)\n",
      "  Crisis:      66,748 (30.8%)\n",
      "  Non-crisis:  95,676 (44.2%)\n",
      "\n",
      "Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Memory usage: 115.94 MB\n",
      "\n",
      "First 10 rows source distribution (showing shuffle worked):\n",
      "['tokyo_olympics', 'worldcup_2018', 'humaid', 'humaid', 'tokyo_olympics', 'humaid', 'GoEmotions', 'humaid', 'GoEmotions', 'GoEmotions']\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining datasets...\\n\")\n",
    "\n",
    "# Concatenate all datasets\n",
    "df_master = pd.concat([\n",
    "    df_goemotions_std,\n",
    "    df_crisis_std,\n",
    "    df_non_crisis_std\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Combined master dataset: {len(df_master):,} rows\")\n",
    "\n",
    "# Shuffle the dataset so rows are randomized (not grouped by source)\n",
    "print(\"Shuffling dataset to randomize row order...\")\n",
    "df_master = df_master.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n✅ Combined and shuffled master dataset created!\")\n",
    "print(f\"\\nTotal rows: {len(df_master):,}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  GoEmotions:  {len(df_goemotions_std):,} ({len(df_goemotions_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Crisis:      {len(df_crisis_std):,} ({len(df_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Non-crisis:  {len(df_non_crisis_std):,} ({len(df_non_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nColumns: {df_master.columns.tolist()}\")\n",
    "print(f\"\\nMemory usage: {df_master.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "# Show that rows are now mixed\n",
    "print(f\"\\nFirst 10 rows source distribution (showing shuffle worked):\")\n",
    "print(df_master.head(10)['source_dataset'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d310a",
   "metadata": {},
   "source": [
    "## 9. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b43a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Null counts:\n",
      "text                   25\n",
      "emotion_label      162424\n",
      "emotion_name       162424\n",
      "source_dataset          0\n",
      "crisis_label        54263\n",
      "event_type              0\n",
      "event_name              0\n",
      "created_at              0\n",
      "informativeness     43816\n",
      "dtype: int64\n",
      "\n",
      "Text validation:\n",
      "  Null texts: 25\n",
      "  Empty texts: 0\n",
      "\n",
      "Emotion label status:\n",
      "  Labeled (GoEmotions):    54,263 (25.0%)\n",
      "  Unlabeled (Crisis+Non):  162,424 (75.0%)\n",
      "\n",
      "Crisis label distribution:\n",
      "  Crisis (1):      66,748\n",
      "  Non-crisis (0):  95,676\n",
      "  Unlabeled (GoE): 54,263\n",
      "\n",
      "Source dataset distribution:\n",
      "source_dataset\n",
      "GoEmotions         54263\n",
      "humaid             43409\n",
      "crisislex          23339\n",
      "tokyo_olympics     20000\n",
      "worldcup_2018      20000\n",
      "fifa_worldcup      20000\n",
      "game_of_thrones    20000\n",
      "us_election        10000\n",
      "coachella           3846\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "print(f\"\\nNull counts:\")\n",
    "print(df_master.isnull().sum())\n",
    "\n",
    "# Check text column\n",
    "null_text = df_master['text'].isna().sum()\n",
    "empty_text = (df_master['text'] == '').sum()\n",
    "print(f\"\\nText validation:\")\n",
    "print(f\"  Null texts: {null_text}\")\n",
    "print(f\"  Empty texts: {empty_text}\")\n",
    "if null_text == 0 and empty_text == 0:\n",
    "    print(f\"  ✅ All rows have text content\")\n",
    "\n",
    "# Check emotion labels\n",
    "labeled_rows = df_master['emotion_label'].notna().sum()\n",
    "unlabeled_rows = df_master['emotion_label'].isna().sum()\n",
    "print(f\"\\nEmotion label status:\")\n",
    "print(f\"  Labeled (GoEmotions):    {labeled_rows:,} ({labeled_rows/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Unlabeled (Crisis+Non):  {unlabeled_rows:,} ({unlabeled_rows/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "# Check crisis labels\n",
    "crisis_rows = (df_master['crisis_label'] == 1).sum()\n",
    "non_crisis_rows = (df_master['crisis_label'] == 0).sum()\n",
    "unlabeled_crisis = df_master['crisis_label'].isna().sum()\n",
    "print(f\"\\nCrisis label distribution:\")\n",
    "print(f\"  Crisis (1):      {crisis_rows:,}\")\n",
    "print(f\"  Non-crisis (0):  {non_crisis_rows:,}\")\n",
    "print(f\"  Unlabeled (GoE): {unlabeled_crisis:,}\")\n",
    "\n",
    "# Check source distribution\n",
    "print(f\"\\nSource dataset distribution:\")\n",
    "print(df_master['source_dataset'].value_counts())\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3e5d6",
   "metadata": {},
   "source": [
    "## 10. Show Sample Data from Each Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e09420da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from each source:\n",
      "\n",
      "GoEmotions sample (with emotion labels):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Xanax thing is burning itself out.Crims don't give a fuck.</td>\n",
       "      <td>13.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's doing stupid things when you're young. Then there's doing horribly stupid things when yo...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>anger</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>One of my favs was when we were in the playoffs against the Habs and we won all five fights in a...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "6                                       The Xanax thing is burning itself out.Crims don't give a fuck.   \n",
       "8  There's doing stupid things when you're young. Then there's doing horribly stupid things when yo...   \n",
       "9  One of my favs was when we were in the playoffs against the Habs and we won all five fights in a...   \n",
       "\n",
       "   emotion_label emotion_name source_dataset  \n",
       "6           13.0      neutral     GoEmotions  \n",
       "8            2.0        anger     GoEmotions  \n",
       "9           13.0      neutral     GoEmotions  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Watters: Trump Bashed by Left After Obama Golfed During LA Floods | Fox News Insider</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hurricane_harvey_2017_train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Texas: ExxonMobile tank damaged by Hurricane Harvey leaking dangerous pollutants&nbsp;&nbsp;#HurricaneHar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hurricane_harvey_2017_train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Australia is stepping up its assistance to New Zealand after the deadly earthquake on the countr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kaikoura_earthquake_2016_test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "2                 Watters: Trump Bashed by Left After Obama Golfed During LA Floods | Fox News Insider   \n",
       "3  #Texas: ExxonMobile tank damaged by Hurricane Harvey leaking dangerous pollutants  #HurricaneHar...   \n",
       "5  Australia is stepping up its assistance to New Zealand after the deadly earthquake on the countr...   \n",
       "\n",
       "   emotion_label emotion_name                     event_name  crisis_label  \n",
       "2            NaN          NaN    hurricane_harvey_2017_train           1.0  \n",
       "3            NaN          NaN    hurricane_harvey_2017_train           1.0  \n",
       "5            NaN          NaN  kaikoura_earthquake_2016_test           1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Made The ðŸ‡®ðŸ‡³ Flag Fly High In Tokyo And We Are Proud Of You! â¤ï¸\\n #MirabaiChanu!\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tokyo_olympics_2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signs of the World Cup learn to sign goal in British Sign Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BH DtLðŸ“£ðŸ“£and FH cross winners.\\nGotcha 1S. Vamooooooooooooos\\n@keinishikori \\n#Tokyo2020 #T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tokyo_olympics_2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  You Made The ðŸ‡®ðŸ‡³ Flag Fly High In Tokyo And We Are Proud Of You! â¤ï¸\\n #MirabaiChanu!\\n\\...   \n",
       "1                                   signs of the World Cup learn to sign goal in British Sign Language   \n",
       "4  BH DtLðŸ“£ðŸ“£and FH cross winners.\\nGotcha 1S. Vamooooooooooooos\\n@keinishikori \\n#Tokyo2020 #T...   \n",
       "\n",
       "   emotion_label emotion_name           event_name  crisis_label  \n",
       "0            NaN          NaN  tokyo_olympics_2020           0.0  \n",
       "1            NaN          NaN   fifa_worldcup_2018           0.0  \n",
       "4            NaN          NaN  tokyo_olympics_2020           0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample rows from each source:\\n\")\n",
    "\n",
    "print(\"GoEmotions sample (with emotion labels):\")\n",
    "display(df_master[df_master['source_dataset'] == 'GoEmotions'][['text', 'emotion_label', 'emotion_name', 'source_dataset']].head(3))\n",
    "\n",
    "print(\"\\nCrisis sample (emotion labels = NULL):\")\n",
    "crisis_sample = df_master[df_master['crisis_label'] == 1][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(crisis_sample)\n",
    "\n",
    "print(\"\\nNon-crisis sample (emotion labels = NULL):\")\n",
    "non_crisis_sample = df_master[df_master['crisis_label'] == 0][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(non_crisis_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e489f01",
   "metadata": {},
   "source": [
    "## 11. Save Master Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0010022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving master training dataset to master_training_data/master_training_data_v3.csv...\n",
      "\n",
      "================================================================================\n",
      "MASTER DATASET SAVED\n",
      "================================================================================\n",
      "\n",
      "✅ Saved to: master_training_data/master_training_data_v3.csv\n",
      "\n",
      "File size: 36.08 MB\n",
      "Total rows: 216,687\n",
      "Total columns: 9\n",
      "\n",
      "Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save to master_training_data folder\n",
    "output_path = 'master_training_data/master_training_data_v3.csv'\n",
    "\n",
    "print(f\"Saving master training dataset to {output_path}...\\n\")\n",
    "\n",
    "# Ensure created_at is in proper datetime format before saving\n",
    "print(\"Validating date formats...\")\n",
    "df_master['created_at'] = pd.to_datetime(df_master['created_at'], errors='coerce')\n",
    "\n",
    "# Report on date quality\n",
    "dates_valid = df_master['created_at'].notna().sum()\n",
    "dates_missing = df_master['created_at'].isna().sum()\n",
    "print(f\"  Valid dates: {dates_valid:,}\")\n",
    "print(f\"  Missing dates: {dates_missing:,}\")\n",
    "\n",
    "if 'created_at_imputed' in df_master.columns:\n",
    "    dates_imputed = df_master['created_at_imputed'].sum()\n",
    "    print(f\"  Imputed dates: {dates_imputed:,}\")\n",
    "\n",
    "# Convert to ISO format string for CSV\n",
    "df_master['created_at'] = df_master['created_at'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_master.to_csv(output_path, index=False)\n",
    "\n",
    "file_size = Path(output_path).stat().st_size / (1024**2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MASTER DATASET SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✅ Saved to: {output_path}\")\n",
    "print(f\"\\nFile size: {file_size:.2f} MB\")\n",
    "print(f\"Total rows: {len(df_master):,}\")\n",
    "print(f\"Total columns: {len(df_master.columns)}\")\n",
    "print(f\"\\nColumns: {df_master.columns.tolist()}\")\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689328d9",
   "metadata": {},
   "source": [
    "## 12. Create Smaller Sample File for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce99657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created sample file: master_training_data/master_training_sample_10kv2.csv\n",
      "   Rows: 10,000\n",
      "   Size: 1.67 MB\n"
     ]
    }
   ],
   "source": [
    "# Create 10K sample for quick testing\n",
    "sample_size = 10000\n",
    "df_sample = df_master.sample(n=sample_size, random_state=42)\n",
    "\n",
    "sample_path = 'master_training_data/master_training_sample_10kv2.csv'\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "\n",
    "print(f\"✅ Created sample file: {sample_path}\")\n",
    "print(f\"   Rows: {len(df_sample):,}\")\n",
    "print(f\"   Size: {Path(sample_path).stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ee71",
   "metadata": {},
   "source": [
    "## 13. Final Summary & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67521c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 Dataset Composition:\n",
      "   Total rows:          216,687\n",
      "   GoEmotions:          54,263 (with emotion labels)\n",
      "   Crisis events:       66,748 (emotion labels = NULL)\n",
      "   Non-crisis events:   95,676 (SAMPLED, emotion labels = NULL)\n",
      "\n",
      "📁 Files Created:\n",
      "   Main:   master_training_data/master_training_data_v3.csv (36.08 MB)\n",
      "   Sample: master_training_data/master_training_sample_10k.csv\n",
      "\n",
      "🏷️ Emotion Labels:\n",
      "   Labeled rows:    54,263 (GoEmotions - for training)\n",
      "   Unlabeled rows:  162,424 (Crisis + Non-crisis - for prediction)\n",
      "\n",
      "🔧 Schema:\n",
      "   Columns: 9\n",
      "      1. text\n",
      "      2. emotion_label\n",
      "      3. emotion_name\n",
      "      4. source_dataset\n",
      "      5. crisis_label\n",
      "      6. event_type\n",
      "      7. event_name\n",
      "      8. created_at\n",
      "      9. informativeness\n",
      "\n",
      "📋 Next Steps:\n",
      "   1. Train multi-task BERT on this dataset (~217K rows)\n",
      "      - Task 1: Emotion classification (using GoEmotions labels)\n",
      "      - Task 2: Crisis detection (using crisis_label)\n",
      "   2. Apply trained BERT to ORIGINAL FULL datasets:\n",
      "      - Full crisis data: 67K tweets\n",
      "      - Full non-crisis data: 1.5M+ tweets\n",
      "   3. Extract emotion features for ALL tweets\n",
      "   4. Create episodes & hourly aggregations for RL agent\n",
      "   5. Train RL agent on temporal emotion patterns\n",
      "\n",
      "================================================================================\n",
      "✅ PHASE 4 COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Dataset Composition:\")\n",
    "print(f\"   Total rows:          {len(df_master):,}\")\n",
    "print(f\"   GoEmotions:          {len(df_goemotions_std):,} (with emotion labels)\")\n",
    "print(f\"   Crisis events:       {len(df_crisis_std):,} (emotion labels = NULL)\")\n",
    "print(f\"   Non-crisis events:   {len(df_non_crisis_std):,} (SAMPLED, emotion labels = NULL)\")\n",
    "\n",
    "print(f\"\\n📁 Files Created:\")\n",
    "print(f\"   Main:   master_training_data/master_training_data_v3.csv ({file_size:.2f} MB)\")\n",
    "print(f\"   Sample: master_training_data/master_training_sample_10k.csv\")\n",
    "\n",
    "print(f\"\\n🏷️ Emotion Labels:\")\n",
    "print(f\"   Labeled rows:    {labeled_rows:,} (GoEmotions - for training)\")\n",
    "print(f\"   Unlabeled rows:  {unlabeled_rows:,} (Crisis + Non-crisis - for prediction)\")\n",
    "\n",
    "print(f\"\\n🔧 Schema:\")\n",
    "print(f\"   Columns: {len(df_master.columns)}\")\n",
    "for i, col in enumerate(df_master.columns, 1):\n",
    "    print(f\"      {i}. {col}\")\n",
    "\n",
    "print(f\"\\n📋 Next Steps:\")\n",
    "print(f\"   1. Train multi-task BERT on this dataset (~217K rows)\")\n",
    "print(f\"      - Task 1: Emotion classification (using GoEmotions labels)\")\n",
    "print(f\"      - Task 2: Crisis detection (using crisis_label)\")\n",
    "print(f\"   2. Apply trained BERT to ORIGINAL FULL datasets:\")\n",
    "print(f\"      - Full crisis data: 67K tweets\")\n",
    "print(f\"      - Full non-crisis data: 1.5M+ tweets\")\n",
    "print(f\"   3. Extract emotion features for ALL tweets\")\n",
    "print(f\"   4. Create episodes & hourly aggregations for RL agent\")\n",
    "print(f\"   5. Train RL agent on temporal emotion patterns\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ PHASE 4 COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
