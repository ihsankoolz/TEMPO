{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2824755c",
   "metadata": {},
   "source": [
    "# Phase 4: Create Master Training Dataset\n",
    "## Combine GoEmotions + Crisis + Non-Crisis Data\n",
    "\n",
    "This notebook creates a **reduced, balanced dataset for training multi-task BERT**.\n",
    "\n",
    "### What this dataset is for:\n",
    "- **Train BERT** on emotion classification (using GoEmotions labels)\n",
    "- **Train BERT** on crisis detection (using crisis_label)\n",
    "- Smaller dataset (~217K rows) for efficient training\n",
    "\n",
    "### What happens after BERT is trained:\n",
    "1. Apply trained BERT to **ORIGINAL FULL datasets** (1.5M+ non-crisis, 67K crisis)\n",
    "2. Extract emotion features for ALL tweets\n",
    "3. Use these features to create episodes & hourly aggregations for RL agent\n",
    "\n",
    "### Data Sources:\n",
    "- **GoEmotions**: 54K Reddit comments with labeled emotions (for BERT training)\n",
    "- **Crisis**: 67K crisis tweets (all kept)\n",
    "- **Non-Crisis**: ~96K sampled non-crisis tweets (reduced from 1.5M for balanced training)\n",
    "\n",
    "### Sampling Strategy:\n",
    "- Non-crisis data is randomly sampled with sports emphasis\n",
    "- Dataset is shuffled so rows are randomized (not grouped by source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9085155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378adf6",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f45d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "1. GoEmotions (with 13 emotions)...\n",
      "   ✓ Loaded 54,263 rows\n",
      "   Columns: ['text', 'emotion_label', 'emotion_name', 'id', 'labels']\n",
      "\n",
      "2. Crisis data (with emotion columns)...\n",
      "   ✓ Loaded 66,748 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'informativeness', 'emotion_label', 'emotion_name']\n",
      "\n",
      "3. Non-crisis data (with emotion columns)...\n",
      "   ✓ Loaded 1,533,696 rows\n",
      "   Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'emotion_label', 'emotion_name']\n",
      "\n",
      "================================================================================\n",
      "Total rows to combine: 1,654,707\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "# Load GoEmotions with 13 emotions\n",
    "print(\"1. GoEmotions (with 13 emotions)...\")\n",
    "df_goemotions = pd.read_csv('goemotion_data/goemotions_with_13_emotions.csv')\n",
    "print(f\"   ✓ Loaded {len(df_goemotions):,} rows\")\n",
    "print(f\"   Columns: {df_goemotions.columns.tolist()}\")\n",
    "\n",
    "# Load crisis data with emotion columns\n",
    "print(\"\\n2. Crisis data (with emotion columns)...\")\n",
    "df_crisis = pd.read_csv('standardized_data/crisis_combined_with_emotions.csv')\n",
    "print(f\"   ✓ Loaded {len(df_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_crisis.columns.tolist()}\")\n",
    "\n",
    "# Load non-crisis data with emotion columns\n",
    "print(\"\\n3. Non-crisis data (with emotion columns)...\")\n",
    "df_non_crisis = pd.read_csv('standardized_data/non_crisis_combined_with_emotions.csv')\n",
    "print(f\"   ✓ Loaded {len(df_non_crisis):,} rows\")\n",
    "print(f\"   Columns: {df_non_crisis.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total rows to combine: {len(df_goemotions) + len(df_crisis) + len(df_non_crisis):,}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ogmggp72bb",
   "metadata": {},
   "source": [
    "## 1.5 Sample Non-Crisis Data (Reduce Dataset Size)\n",
    "\n",
    "Downsample non-crisis data from 1.5M to ~100K rows:\n",
    "- Keep all GoEmotions (54K) - needed for training\n",
    "- Keep all Crisis (67K) - core data for crisis detection\n",
    "- Sample non-crisis to ~100K with sports emphasis\n",
    "\n",
    "**Sampling Distribution:**\n",
    "| Source | Type | Sample Size |\n",
    "|--------|------|-------------|\n",
    "| worldcup_2018 | Sports | 20,000 |\n",
    "| tokyo_olympics | Sports | 20,000 |\n",
    "| fifa_worldcup | Sports | 20,000 |\n",
    "| game_of_thrones | Entertainment | 20,000 |\n",
    "| us_election | Politics | 10,000 |\n",
    "| coachella | Entertainment | All (~3,846) |\n",
    "| music_concerts | Entertainment | All (~1,830) |\n",
    "\n",
    "**Rationale:** Sports-heavy distribution helps model learn to distinguish sports excitement from crisis fear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dforzd1aoro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling non-crisis data...\n",
      "\n",
      "Original non-crisis distribution:\n",
      "source_dataset\n",
      "worldcup_2018      20000\n",
      "tokyo_olympics     20000\n",
      "fifa_worldcup      20000\n",
      "game_of_thrones    20000\n",
      "us_election        10000\n",
      "coachella           3846\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total before sampling: 95,676\n",
      "  worldcup_2018: 20,000 (kept all)\n",
      "  tokyo_olympics: 20,000 (kept all)\n",
      "  fifa_worldcup: 20,000 (kept all)\n",
      "  game_of_thrones: 20,000 (kept all)\n",
      "  us_election: 10,000 (kept all)\n",
      "  coachella: 3,846 (kept all)\n",
      "  music_concerts: 1,830 (kept all)\n",
      "\n",
      "============================================================\n",
      "Non-crisis sampling complete!\n",
      "  Before: 95,676 rows\n",
      "  After:  95,676 rows\n",
      "  Reduction: 0.0%\n",
      "============================================================\n",
      "\n",
      "New non-crisis distribution:\n",
      "source_dataset\n",
      "worldcup_2018      20000\n",
      "tokyo_olympics     20000\n",
      "fifa_worldcup      20000\n",
      "game_of_thrones    20000\n",
      "us_election        10000\n",
      "coachella           3846\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampling non-crisis data...\\n\")\n",
    "\n",
    "# Define sampling configuration\n",
    "SAMPLE_CONFIG = {\n",
    "    'worldcup_2018': 20000,      # Sports\n",
    "    'tokyo_olympics': 20000,     # Sports\n",
    "    'fifa_worldcup': 20000,      # Sports\n",
    "    'game_of_thrones': 20000,    # Entertainment\n",
    "    'us_election': 10000,        # Politics\n",
    "    'coachella': None,           # Keep all (small dataset)\n",
    "    'music_concerts': None,      # Keep all (small dataset)\n",
    "}\n",
    "\n",
    "print(\"Original non-crisis distribution:\")\n",
    "print(df_non_crisis['source_dataset'].value_counts())\n",
    "print(f\"\\nTotal before sampling: {len(df_non_crisis):,}\")\n",
    "\n",
    "# Sample each source\n",
    "sampled_dfs = []\n",
    "for source, sample_size in SAMPLE_CONFIG.items():\n",
    "    source_df = df_non_crisis[df_non_crisis['source_dataset'] == source]\n",
    "    \n",
    "    if sample_size is None or len(source_df) <= sample_size:\n",
    "        # Keep all rows for small datasets\n",
    "        sampled_dfs.append(source_df)\n",
    "        print(f\"  {source}: {len(source_df):,} (kept all)\")\n",
    "    else:\n",
    "        # Random sample for large datasets\n",
    "        sampled = source_df.sample(n=sample_size, random_state=42)\n",
    "        sampled_dfs.append(sampled)\n",
    "        print(f\"  {source}: {sample_size:,} (sampled from {len(source_df):,})\")\n",
    "\n",
    "# Combine sampled data\n",
    "df_non_crisis_sampled = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Non-crisis sampling complete!\")\n",
    "print(f\"  Before: {len(df_non_crisis):,} rows\")\n",
    "print(f\"  After:  {len(df_non_crisis_sampled):,} rows\")\n",
    "print(f\"  Reduction: {(1 - len(df_non_crisis_sampled)/len(df_non_crisis))*100:.1f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Replace original with sampled version\n",
    "df_non_crisis = df_non_crisis_sampled\n",
    "\n",
    "print(\"\\nNew non-crisis distribution:\")\n",
    "print(df_non_crisis['source_dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bdfd9",
   "metadata": {},
   "source": [
    "## 2. Check Current Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b68ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current column schemas:\n",
      "\n",
      "GoEmotions columns:\n",
      "  - text: object\n",
      "  - emotion_label: int64\n",
      "  - emotion_name: object\n",
      "  - id: object\n",
      "  - labels: object\n",
      "\n",
      "Crisis columns:\n",
      "  - text: object\n",
      "  - created_at: object\n",
      "  - event_name: object\n",
      "  - event_type: object\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: object\n",
      "  - informativeness: object\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n",
      "\n",
      "Non-crisis columns:\n",
      "  - text: object\n",
      "  - created_at: object\n",
      "  - event_name: object\n",
      "  - event_type: object\n",
      "  - crisis_label: int64\n",
      "  - source_dataset: object\n",
      "  - emotion_label: float64\n",
      "  - emotion_name: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Current column schemas:\\n\")\n",
    "\n",
    "print(\"GoEmotions columns:\")\n",
    "for col in df_goemotions.columns:\n",
    "    print(f\"  - {col}: {df_goemotions[col].dtype}\")\n",
    "\n",
    "print(\"\\nCrisis columns:\")\n",
    "for col in df_crisis.columns:\n",
    "    print(f\"  - {col}: {df_crisis[col].dtype}\")\n",
    "\n",
    "print(\"\\nNon-crisis columns:\")\n",
    "for col in df_non_crisis.columns:\n",
    "    print(f\"  - {col}: {df_non_crisis[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d6cb2",
   "metadata": {},
   "source": [
    "## 3. Define Master Schema\n",
    "\n",
    "Create unified column structure for all datasets:\n",
    "- **text**: Tweet/comment text\n",
    "- **emotion_label**: Numeric emotion (1-13, NULL for unlabeled)\n",
    "- **emotion_name**: Text emotion name (NULL for unlabeled)\n",
    "- **source_dataset**: Origin of data (GoEmotions, HumAID, CrisisLex, etc.)\n",
    "- **crisis_label**: Binary (1=crisis, 0=non-crisis, NULL for GoEmotions)\n",
    "- **event_type**: General category (hurricane, sports, etc., NULL for GoEmotions)\n",
    "- **event_name**: Specific event (hurricane_harvey_2017, etc., NULL for GoEmotions)\n",
    "- **created_at**: Timestamp (NULL for GoEmotions)\n",
    "- **informativeness**: CrisisLex informativeness label (NULL for others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403345da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master schema columns:\n",
      "  1. text\n",
      "  2. emotion_label\n",
      "  3. emotion_name\n",
      "  4. source_dataset\n",
      "  5. crisis_label\n",
      "  6. event_type\n",
      "  7. event_name\n",
      "  8. created_at\n",
      "  9. informativeness\n"
     ]
    }
   ],
   "source": [
    "# Define master column set\n",
    "MASTER_COLUMNS = [\n",
    "    'text',\n",
    "    'emotion_label',\n",
    "    'emotion_name',\n",
    "    'source_dataset',\n",
    "    'crisis_label',\n",
    "    'event_type',\n",
    "    'event_name',\n",
    "    'created_at',\n",
    "    'informativeness'\n",
    "]\n",
    "\n",
    "print(\"Master schema columns:\")\n",
    "for i, col in enumerate(MASTER_COLUMNS, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec4a07",
   "metadata": {},
   "source": [
    "## 4. Standardize GoEmotions Data\n",
    "\n",
    "Add missing columns to GoEmotions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311e49b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing GoEmotions data...\n",
      "\n",
      "✓ GoEmotions standardized: 54,263 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to cook myself.</td>\n",
       "      <td>13</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will think hes having a laugh screwing with people instead ...</td>\n",
       "      <td>13</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>GoEmotions</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                                          My favourite food is anything I didn't have to cook myself.   \n",
       "1  Now if he does off himself, everyone will think hes having a laugh screwing with people instead ...   \n",
       "2                                                                       WHY THE FUCK IS BAYLESS ISOING   \n",
       "\n",
       "   emotion_label emotion_name source_dataset  crisis_label event_type  \\\n",
       "0             13      neutral     GoEmotions           NaN              \n",
       "1             13      neutral     GoEmotions           NaN              \n",
       "2              2        anger     GoEmotions           NaN              \n",
       "\n",
       "  event_name created_at informativeness  \n",
       "0                                        \n",
       "1                                        \n",
       "2                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing GoEmotions data...\\n\")\n",
    "\n",
    "# Create standardized GoEmotions dataframe\n",
    "df_goemotions_std = pd.DataFrame()\n",
    "\n",
    "# Keep existing columns\n",
    "df_goemotions_std['text'] = df_goemotions['text']\n",
    "df_goemotions_std['emotion_label'] = df_goemotions['emotion_label']\n",
    "df_goemotions_std['emotion_name'] = df_goemotions['emotion_name']\n",
    "\n",
    "# Add source\n",
    "df_goemotions_std['source_dataset'] = 'GoEmotions'\n",
    "\n",
    "# Add NULL columns (GoEmotions is not crisis-related)\n",
    "df_goemotions_std['crisis_label'] = np.nan\n",
    "df_goemotions_std['event_type'] = ''\n",
    "df_goemotions_std['event_name'] = ''\n",
    "df_goemotions_std['created_at'] = ''\n",
    "df_goemotions_std['informativeness'] = ''\n",
    "\n",
    "print(f\"✓ GoEmotions standardized: {len(df_goemotions_std):,} rows\")\n",
    "print(f\"  Columns: {df_goemotions_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_goemotions_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cfc574",
   "metadata": {},
   "source": [
    "## 5. Standardize Crisis Data\n",
    "\n",
    "Select and reorder crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0969d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing crisis data...\n",
      "\n",
      "✓ Crisis standardized: 66,748 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@GreenABEnergy How can @AirworksCanada assist in the cleanup? #AlbertaStrong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humaid</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>canada_wildfires_2016_dev</td>\n",
       "      <td>2016-05-19 18:16:11.727000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @katvondawn: Thoughts &amp;amp; prayers going to all those being affected by the wildfire in Cana...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humaid</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>canada_wildfires_2016_dev</td>\n",
       "      <td>2016-05-09 03:58:37.448000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Glacier Farm Media pledges $50K in support for Fort McMurray wildfire disaster relief.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humaid</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>canada_wildfires_2016_dev</td>\n",
       "      <td>2016-05-12 12:41:05.044000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                        .@GreenABEnergy How can @AirworksCanada assist in the cleanup? #AlbertaStrong   \n",
       "1  RT @katvondawn: Thoughts &amp; prayers going to all those being affected by the wildfire in Cana...   \n",
       "2               Glacier Farm Media pledges $50K in support for Fort McMurray wildfire disaster relief.   \n",
       "\n",
       "   emotion_label  emotion_name source_dataset  crisis_label event_type  \\\n",
       "0            NaN           NaN         humaid             1   wildfire   \n",
       "1            NaN           NaN         humaid             1   wildfire   \n",
       "2            NaN           NaN         humaid             1   wildfire   \n",
       "\n",
       "                  event_name                        created_at informativeness  \n",
       "0  canada_wildfires_2016_dev  2016-05-19 18:16:11.727000+00:00             NaN  \n",
       "1  canada_wildfires_2016_dev  2016-05-09 03:58:37.448000+00:00             NaN  \n",
       "2  canada_wildfires_2016_dev  2016-05-12 12:41:05.044000+00:00             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing crisis data...\\n\")\n",
    "\n",
    "# Create standardized crisis dataframe\n",
    "df_crisis_std = pd.DataFrame()\n",
    "\n",
    "df_crisis_std['text'] = df_crisis['text']\n",
    "df_crisis_std['emotion_label'] = df_crisis['emotion_label']  # Will be NaN\n",
    "df_crisis_std['emotion_name'] = df_crisis['emotion_name']    # Will be empty\n",
    "df_crisis_std['source_dataset'] = df_crisis['source_dataset']\n",
    "df_crisis_std['crisis_label'] = df_crisis['crisis_label']\n",
    "df_crisis_std['event_type'] = df_crisis['event_type']\n",
    "df_crisis_std['event_name'] = df_crisis['event_name']\n",
    "df_crisis_std['created_at'] = df_crisis['created_at']\n",
    "df_crisis_std['informativeness'] = df_crisis['informativeness']\n",
    "\n",
    "print(f\"✓ Crisis standardized: {len(df_crisis_std):,} rows\")\n",
    "print(f\"  Columns: {df_crisis_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_crisis_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702522d",
   "metadata": {},
   "source": [
    "## 6. Standardize Non-Crisis Data\n",
    "\n",
    "Select and reorder non-crisis columns to match master schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd741cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing non-crisis data...\n",
      "\n",
      "✓ Non-crisis standardized: 95,676 rows\n",
      "  Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>crisis_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>informativeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thibuat Courtois Winner Golden Glove Fifa World Cup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worldcup_2018</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>2018-07-15 17:38:07</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paolo Dybala scored more goals for Juventus last season than he played minutes at the Goals For ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worldcup_2018</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>2018-06-30 16:55:14</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France have won the FIFA in Moscow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worldcup_2018</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>2018-07-15 17:56:43</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                                                  Thibuat Courtois Winner Golden Glove Fifa World Cup   \n",
       "1  Paolo Dybala scored more goals for Juventus last season than he played minutes at the Goals For ...   \n",
       "2                                                                   France have won the FIFA in Moscow   \n",
       "\n",
       "   emotion_label  emotion_name source_dataset  crisis_label event_type  \\\n",
       "0            NaN           NaN  worldcup_2018             0     sports   \n",
       "1            NaN           NaN  worldcup_2018             0     sports   \n",
       "2            NaN           NaN  worldcup_2018             0     sports   \n",
       "\n",
       "           event_name           created_at informativeness  \n",
       "0  fifa_worldcup_2018  2018-07-15 17:38:07                  \n",
       "1  fifa_worldcup_2018  2018-06-30 16:55:14                  \n",
       "2  fifa_worldcup_2018  2018-07-15 17:56:43                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardizing non-crisis data...\\n\")\n",
    "\n",
    "# Create standardized non-crisis dataframe\n",
    "df_non_crisis_std = pd.DataFrame()\n",
    "\n",
    "df_non_crisis_std['text'] = df_non_crisis['text']\n",
    "df_non_crisis_std['emotion_label'] = df_non_crisis['emotion_label']  # Will be NaN\n",
    "df_non_crisis_std['emotion_name'] = df_non_crisis['emotion_name']    # Will be empty\n",
    "df_non_crisis_std['source_dataset'] = df_non_crisis['source_dataset']\n",
    "df_non_crisis_std['crisis_label'] = df_non_crisis['crisis_label']\n",
    "df_non_crisis_std['event_type'] = df_non_crisis['event_type']\n",
    "df_non_crisis_std['event_name'] = df_non_crisis['event_name']\n",
    "df_non_crisis_std['created_at'] = df_non_crisis['created_at']\n",
    "\n",
    "# Non-crisis doesn't have informativeness\n",
    "df_non_crisis_std['informativeness'] = ''\n",
    "\n",
    "print(f\"✓ Non-crisis standardized: {len(df_non_crisis_std):,} rows\")\n",
    "print(f\"  Columns: {df_non_crisis_std.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(df_non_crisis_std.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3fcb7",
   "metadata": {},
   "source": [
    "## 7. Validate Schema Alignment\n",
    "\n",
    "Ensure all three datasets have identical column structure before combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec744ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SCHEMA VALIDATION\n",
      "================================================================================\n",
      "\n",
      "GoEmotions columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "Crisis columns:     ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "Non-crisis columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "✅ All datasets have matching column structure!\n",
      "\n",
      "✅ Columns match master schema!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SCHEMA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check column names\n",
    "goemotions_cols = df_goemotions_std.columns.tolist()\n",
    "crisis_cols = df_crisis_std.columns.tolist()\n",
    "non_crisis_cols = df_non_crisis_std.columns.tolist()\n",
    "\n",
    "print(f\"\\nGoEmotions columns: {goemotions_cols}\")\n",
    "print(f\"Crisis columns:     {crisis_cols}\")\n",
    "print(f\"Non-crisis columns: {non_crisis_cols}\")\n",
    "\n",
    "# Validate all match\n",
    "if goemotions_cols == crisis_cols == non_crisis_cols:\n",
    "    print(\"\\n✅ All datasets have matching column structure!\")\n",
    "else:\n",
    "    print(\"\\n❌ Column mismatch detected!\")\n",
    "    print(f\"\\nDifferences:\")\n",
    "    if goemotions_cols != crisis_cols:\n",
    "        print(f\"  GoEmotions vs Crisis: {set(goemotions_cols) ^ set(crisis_cols)}\")\n",
    "    if crisis_cols != non_crisis_cols:\n",
    "        print(f\"  Crisis vs Non-crisis: {set(crisis_cols) ^ set(non_crisis_cols)}\")\n",
    "\n",
    "# Check if columns match master schema\n",
    "if goemotions_cols == MASTER_COLUMNS:\n",
    "    print(\"\\n✅ Columns match master schema!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Column order differs from master schema\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add23f7",
   "metadata": {},
   "source": [
    "## 8. Combine All Datasets\n",
    "\n",
    "Concatenate all three standardized datasets into master training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba252696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "\n",
      "Combined master dataset: 216,687 rows\n",
      "Shuffling dataset to randomize row order...\n",
      "\n",
      "✅ Combined and shuffled master dataset created!\n",
      "\n",
      "Total rows: 216,687\n",
      "\n",
      "Breakdown:\n",
      "  GoEmotions:  54,263 (25.0%)\n",
      "  Crisis:      66,748 (30.8%)\n",
      "  Non-crisis:  95,676 (44.2%)\n",
      "\n",
      "Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "Memory usage: 116.61 MB\n",
      "\n",
      "First 10 rows source distribution (showing shuffle worked):\n",
      "['tokyo_olympics', 'worldcup_2018', 'humaid', 'humaid', 'tokyo_olympics', 'humaid', 'GoEmotions', 'humaid', 'GoEmotions', 'GoEmotions']\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining datasets...\\n\")\n",
    "\n",
    "# Concatenate all datasets\n",
    "df_master = pd.concat([\n",
    "    df_goemotions_std,\n",
    "    df_crisis_std,\n",
    "    df_non_crisis_std\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Combined master dataset: {len(df_master):,} rows\")\n",
    "\n",
    "# Shuffle the dataset so rows are randomized (not grouped by source)\n",
    "print(\"Shuffling dataset to randomize row order...\")\n",
    "df_master = df_master.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n✅ Combined and shuffled master dataset created!\")\n",
    "print(f\"\\nTotal rows: {len(df_master):,}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  GoEmotions:  {len(df_goemotions_std):,} ({len(df_goemotions_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Crisis:      {len(df_crisis_std):,} ({len(df_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Non-crisis:  {len(df_non_crisis_std):,} ({len(df_non_crisis_std)/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nColumns: {df_master.columns.tolist()}\")\n",
    "print(f\"\\nMemory usage: {df_master.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "# Show that rows are now mixed\n",
    "print(f\"\\nFirst 10 rows source distribution (showing shuffle worked):\")\n",
    "print(df_master.head(10)['source_dataset'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d310a",
   "metadata": {},
   "source": [
    "## 9. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b43a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Null counts:\n",
      "text                   25\n",
      "emotion_label      162424\n",
      "emotion_name       162424\n",
      "source_dataset          0\n",
      "crisis_label        54263\n",
      "event_type              0\n",
      "event_name              0\n",
      "created_at              0\n",
      "informativeness     43816\n",
      "dtype: int64\n",
      "\n",
      "Text validation:\n",
      "  Null texts: 25\n",
      "  Empty texts: 0\n",
      "\n",
      "Emotion label status:\n",
      "  Labeled (GoEmotions):    54,263 (25.0%)\n",
      "  Unlabeled (Crisis+Non):  162,424 (75.0%)\n",
      "\n",
      "Crisis label distribution:\n",
      "  Crisis (1):      66,748\n",
      "  Non-crisis (0):  95,676\n",
      "  Unlabeled (GoE): 54,263\n",
      "\n",
      "Source dataset distribution:\n",
      "source_dataset\n",
      "GoEmotions         54263\n",
      "humaid             43409\n",
      "crisislex          23339\n",
      "tokyo_olympics     20000\n",
      "worldcup_2018      20000\n",
      "fifa_worldcup      20000\n",
      "game_of_thrones    20000\n",
      "us_election        10000\n",
      "coachella           3846\n",
      "music_concerts      1830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "print(f\"\\nNull counts:\")\n",
    "print(df_master.isnull().sum())\n",
    "\n",
    "# Check text column\n",
    "null_text = df_master['text'].isna().sum()\n",
    "empty_text = (df_master['text'] == '').sum()\n",
    "print(f\"\\nText validation:\")\n",
    "print(f\"  Null texts: {null_text}\")\n",
    "print(f\"  Empty texts: {empty_text}\")\n",
    "if null_text == 0 and empty_text == 0:\n",
    "    print(f\"  ✅ All rows have text content\")\n",
    "\n",
    "# Check emotion labels\n",
    "labeled_rows = df_master['emotion_label'].notna().sum()\n",
    "unlabeled_rows = df_master['emotion_label'].isna().sum()\n",
    "print(f\"\\nEmotion label status:\")\n",
    "print(f\"  Labeled (GoEmotions):    {labeled_rows:,} ({labeled_rows/len(df_master)*100:.1f}%)\")\n",
    "print(f\"  Unlabeled (Crisis+Non):  {unlabeled_rows:,} ({unlabeled_rows/len(df_master)*100:.1f}%)\")\n",
    "\n",
    "# Check crisis labels\n",
    "crisis_rows = (df_master['crisis_label'] == 1).sum()\n",
    "non_crisis_rows = (df_master['crisis_label'] == 0).sum()\n",
    "unlabeled_crisis = df_master['crisis_label'].isna().sum()\n",
    "print(f\"\\nCrisis label distribution:\")\n",
    "print(f\"  Crisis (1):      {crisis_rows:,}\")\n",
    "print(f\"  Non-crisis (0):  {non_crisis_rows:,}\")\n",
    "print(f\"  Unlabeled (GoE): {unlabeled_crisis:,}\")\n",
    "\n",
    "# Check source distribution\n",
    "print(f\"\\nSource dataset distribution:\")\n",
    "print(df_master['source_dataset'].value_counts())\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3e5d6",
   "metadata": {},
   "source": [
    "## 10. Show Sample Data from Each Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09420da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from each source:\n",
      "\n",
      "GoEmotions sample (with emotion labels):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>source_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Xanax thing is burning itself out.Crims don't give a fuck.</td>\n",
       "      <td>13.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's doing stupid things when you're young. Then there's doing horribly stupid things when yo...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>anger</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>One of my favs was when we were in the playoffs against the Habs and we won all five fights in a...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>GoEmotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "6                                       The Xanax thing is burning itself out.Crims don't give a fuck.   \n",
       "8  There's doing stupid things when you're young. Then there's doing horribly stupid things when yo...   \n",
       "9  One of my favs was when we were in the playoffs against the Habs and we won all five fights in a...   \n",
       "\n",
       "   emotion_label emotion_name source_dataset  \n",
       "6           13.0      neutral     GoEmotions  \n",
       "8            2.0        anger     GoEmotions  \n",
       "9           13.0      neutral     GoEmotions  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@CDCgov @ltgrusselhonore Record rainfall from #HurricaneHarvey should flooded areas of #Texas be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hurricane_harvey_2017_dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just a small part. Not even a drop in the bucket. Millions are out of their homes.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hurricane_harvey_2017_dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fort McMurray Wildfire Relief Effort Donations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canada_wildfires_2016_train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "2  @CDCgov @ltgrusselhonore Record rainfall from #HurricaneHarvey should flooded areas of #Texas be...   \n",
       "3                   Just a small part. Not even a drop in the bucket. Millions are out of their homes.   \n",
       "5                                                       Fort McMurray Wildfire Relief Effort Donations   \n",
       "\n",
       "   emotion_label emotion_name                   event_name  crisis_label  \n",
       "2            NaN          NaN    hurricane_harvey_2017_dev           1.0  \n",
       "3            NaN          NaN    hurricane_harvey_2017_dev           1.0  \n",
       "5            NaN          NaN  canada_wildfires_2016_train           1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-crisis sample (emotion labels = NULL):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_name</th>\n",
       "      <th>event_name</th>\n",
       "      <th>crisis_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Made The ðŸ‡®ðŸ‡³ Flag Fly High In Tokyo And We Are Proud Of You! â¤ï¸\\n #MirabaiChanu!\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tokyo_olympics_2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signs of the World Cup learn to sign goal in British Sign Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fifa_worldcup_2018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BH DtLðŸ“£ðŸ“£and FH cross winners.\\nGotcha 1S. Vamooooooooooooos\\n@keinishikori \\n#Tokyo2020 #T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tokyo_olympics_2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  You Made The ðŸ‡®ðŸ‡³ Flag Fly High In Tokyo And We Are Proud Of You! â¤ï¸\\n #MirabaiChanu!\\n\\...   \n",
       "1                                   signs of the World Cup learn to sign goal in British Sign Language   \n",
       "4  BH DtLðŸ“£ðŸ“£and FH cross winners.\\nGotcha 1S. Vamooooooooooooos\\n@keinishikori \\n#Tokyo2020 #T...   \n",
       "\n",
       "   emotion_label emotion_name           event_name  crisis_label  \n",
       "0            NaN          NaN  tokyo_olympics_2020           0.0  \n",
       "1            NaN          NaN   fifa_worldcup_2018           0.0  \n",
       "4            NaN          NaN  tokyo_olympics_2020           0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample rows from each source:\\n\")\n",
    "\n",
    "print(\"GoEmotions sample (with emotion labels):\")\n",
    "display(df_master[df_master['source_dataset'] == 'GoEmotions'][['text', 'emotion_label', 'emotion_name', 'source_dataset']].head(3))\n",
    "\n",
    "print(\"\\nCrisis sample (emotion labels = NULL):\")\n",
    "crisis_sample = df_master[df_master['crisis_label'] == 1][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(crisis_sample)\n",
    "\n",
    "print(\"\\nNon-crisis sample (emotion labels = NULL):\")\n",
    "non_crisis_sample = df_master[df_master['crisis_label'] == 0][['text', 'emotion_label', 'emotion_name', 'event_name', 'crisis_label']].head(3)\n",
    "display(non_crisis_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e489f01",
   "metadata": {},
   "source": [
    "## 11. Save Master Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0010022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving master training dataset to master_training_data/master_training_data_v3.csv...\n",
      "\n",
      "================================================================================\n",
      "MASTER DATASET SAVED\n",
      "================================================================================\n",
      "\n",
      "✅ Saved to: master_training_data/master_training_data_v3.csv\n",
      "\n",
      "File size: 36.96 MB\n",
      "Total rows: 216,687\n",
      "Total columns: 9\n",
      "\n",
      "Columns: ['text', 'emotion_label', 'emotion_name', 'source_dataset', 'crisis_label', 'event_type', 'event_name', 'created_at', 'informativeness']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save to master_training_data folder\n",
    "output_path = 'master_training_data/master_training_data_v3.csv'\n",
    "\n",
    "print(f\"Saving master training dataset to {output_path}...\\n\")\n",
    "df_master.to_csv(output_path, index=False)\n",
    "\n",
    "file_size = Path(output_path).stat().st_size / (1024**2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MASTER DATASET SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✅ Saved to: {output_path}\")\n",
    "print(f\"\\nFile size: {file_size:.2f} MB\")\n",
    "print(f\"Total rows: {len(df_master):,}\")\n",
    "print(f\"Total columns: {len(df_master.columns)}\")\n",
    "print(f\"\\nColumns: {df_master.columns.tolist()}\")\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689328d9",
   "metadata": {},
   "source": [
    "## 12. Create Smaller Sample File for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce99657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created sample file: master_training_data/master_training_sample_10kv2.csv\n",
      "   Rows: 10,000\n",
      "   Size: 1.71 MB\n"
     ]
    }
   ],
   "source": [
    "# Create 10K sample for quick testing\n",
    "sample_size = 10000\n",
    "df_sample = df_master.sample(n=sample_size, random_state=42)\n",
    "\n",
    "sample_path = 'master_training_data/master_training_sample_10kv2.csv'\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "\n",
    "print(f\"✅ Created sample file: {sample_path}\")\n",
    "print(f\"   Rows: {len(df_sample):,}\")\n",
    "print(f\"   Size: {Path(sample_path).stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ee71",
   "metadata": {},
   "source": [
    "## 13. Final Summary & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67521c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 Dataset Composition:\n",
      "   Total rows:          216,687\n",
      "   GoEmotions:          54,263 (with emotion labels)\n",
      "   Crisis events:       66,748 (emotion labels = NULL)\n",
      "   Non-crisis events:   95,676 (SAMPLED, emotion labels = NULL)\n",
      "\n",
      "📁 Files Created:\n",
      "   Main:   master_training_data/master_training_data_v3.csv (36.96 MB)\n",
      "   Sample: master_training_data/master_training_sample_10k.csv\n",
      "\n",
      "🏷️ Emotion Labels:\n",
      "   Labeled rows:    54,263 (GoEmotions - for training)\n",
      "   Unlabeled rows:  162,424 (Crisis + Non-crisis - for prediction)\n",
      "\n",
      "🔧 Schema:\n",
      "   Columns: 9\n",
      "      1. text\n",
      "      2. emotion_label\n",
      "      3. emotion_name\n",
      "      4. source_dataset\n",
      "      5. crisis_label\n",
      "      6. event_type\n",
      "      7. event_name\n",
      "      8. created_at\n",
      "      9. informativeness\n",
      "\n",
      "📋 Next Steps:\n",
      "   1. Train multi-task BERT on this dataset (~217K rows)\n",
      "      - Task 1: Emotion classification (using GoEmotions labels)\n",
      "      - Task 2: Crisis detection (using crisis_label)\n",
      "   2. Apply trained BERT to ORIGINAL FULL datasets:\n",
      "      - Full crisis data: 67K tweets\n",
      "      - Full non-crisis data: 1.5M+ tweets\n",
      "   3. Extract emotion features for ALL tweets\n",
      "   4. Create episodes & hourly aggregations for RL agent\n",
      "   5. Train RL agent on temporal emotion patterns\n",
      "\n",
      "================================================================================\n",
      "✅ PHASE 4 COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Dataset Composition:\")\n",
    "print(f\"   Total rows:          {len(df_master):,}\")\n",
    "print(f\"   GoEmotions:          {len(df_goemotions_std):,} (with emotion labels)\")\n",
    "print(f\"   Crisis events:       {len(df_crisis_std):,} (emotion labels = NULL)\")\n",
    "print(f\"   Non-crisis events:   {len(df_non_crisis_std):,} (SAMPLED, emotion labels = NULL)\")\n",
    "\n",
    "print(f\"\\n📁 Files Created:\")\n",
    "print(f\"   Main:   master_training_data/master_training_data_v3.csv ({file_size:.2f} MB)\")\n",
    "print(f\"   Sample: master_training_data/master_training_sample_10k.csv\")\n",
    "\n",
    "print(f\"\\n🏷️ Emotion Labels:\")\n",
    "print(f\"   Labeled rows:    {labeled_rows:,} (GoEmotions - for training)\")\n",
    "print(f\"   Unlabeled rows:  {unlabeled_rows:,} (Crisis + Non-crisis - for prediction)\")\n",
    "\n",
    "print(f\"\\n🔧 Schema:\")\n",
    "print(f\"   Columns: {len(df_master.columns)}\")\n",
    "for i, col in enumerate(df_master.columns, 1):\n",
    "    print(f\"      {i}. {col}\")\n",
    "\n",
    "print(f\"\\n📋 Next Steps:\")\n",
    "print(f\"   1. Train multi-task BERT on this dataset (~217K rows)\")\n",
    "print(f\"      - Task 1: Emotion classification (using GoEmotions labels)\")\n",
    "print(f\"      - Task 2: Crisis detection (using crisis_label)\")\n",
    "print(f\"   2. Apply trained BERT to ORIGINAL FULL datasets:\")\n",
    "print(f\"      - Full crisis data: 67K tweets\")\n",
    "print(f\"      - Full non-crisis data: 1.5M+ tweets\")\n",
    "print(f\"   3. Extract emotion features for ALL tweets\")\n",
    "print(f\"   4. Create episodes & hourly aggregations for RL agent\")\n",
    "print(f\"   5. Train RL agent on temporal emotion patterns\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ PHASE 4 COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
