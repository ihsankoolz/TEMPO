{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6b1cb9",
   "metadata": {},
   "source": [
    "# üìä TEMPO Project Status Dashboard\n",
    "## Visual Overview of Data Files and Structure\n",
    "\n",
    "This notebook provides a complete visual summary of:\n",
    "- What data folders exist\n",
    "- File sizes and row counts\n",
    "- Column structures\n",
    "- What's missing vs. what you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd622211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ef0e0",
   "metadata": {},
   "source": [
    "## 1. üìÇ Directory Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected data folders\n",
    "expected_folders = [\n",
    "    'baseline_data',\n",
    "    'crisis_datasets', \n",
    "    'goemotion_data',\n",
    "    'master_training_data',\n",
    "    'non_crisis_data',\n",
    "    'standardized_data'\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA FOLDER STATUS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "folder_status = {}\n",
    "for folder in expected_folders:\n",
    "    exists = Path(folder).exists()\n",
    "    status = \"‚úÖ EXISTS\" if exists else \"‚ùå MISSING\"\n",
    "    folder_status[folder] = exists\n",
    "    \n",
    "    # Get size if exists\n",
    "    if exists:\n",
    "        total_size = sum(f.stat().st_size for f in Path(folder).rglob('*') if f.is_file())\n",
    "        size_mb = total_size / (1024 * 1024)\n",
    "        print(f\"{status}  {folder:<30} ({size_mb:>8.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"{status}  {folder:<30} (NOT DOWNLOADED)\")\n",
    "\n",
    "print()\n",
    "print(f\"Summary: {sum(folder_status.values())}/{len(expected_folders)} folders present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6405c7",
   "metadata": {},
   "source": [
    "## 2. üìÑ CSV Files Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files\n",
    "csv_files = list(Path('.').rglob('*.csv'))\n",
    "\n",
    "# Filter out git and venv folders\n",
    "csv_files = [f for f in csv_files if '.git' not in str(f) and 'venv' not in str(f) and '.venv' not in str(f)]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"FOUND {len(csv_files)} CSV FILES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Group by folder\n",
    "from collections import defaultdict\n",
    "files_by_folder = defaultdict(list)\n",
    "\n",
    "for f in csv_files:\n",
    "    folder = str(f.parent)\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    files_by_folder[folder].append({\n",
    "        'name': f.name,\n",
    "        'size_mb': size_mb,\n",
    "        'path': str(f)\n",
    "    })\n",
    "\n",
    "# Display by folder\n",
    "for folder in sorted(files_by_folder.keys()):\n",
    "    print(f\"\\nüìÅ {folder}/\")\n",
    "    print(\"-\" * 80)\n",
    "    for file_info in sorted(files_by_folder[folder], key=lambda x: x['name']):\n",
    "        print(f\"   {file_info['name']:<45} {file_info['size_mb']:>8.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a6b63",
   "metadata": {},
   "source": [
    "## 3. üìä Dataset Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key datasets to analyze\n",
    "key_datasets = [\n",
    "    ('standardized_data/crisis_combined.csv', 'Crisis Combined'),\n",
    "    ('standardized_data/non_crisis_combined.csv', 'Non-Crisis Combined'),\n",
    "    ('standardized_data/humaid_standardized.csv', 'HumAID Standardized'),\n",
    "    ('standardized_data/crisislex_standardized.csv', 'CrisisLex Standardized'),\n",
    "    ('goemotion_data/goemotions.csv', 'GoEmotions (27 emotions)'),\n",
    "    ('master_training_data/master_training_data.csv', 'Master Training File'),\n",
    "]\n",
    "\n",
    "dataset_summary = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY DATASETS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for filepath, name in key_datasets:\n",
    "    if Path(filepath).exists():\n",
    "        # Quick check using line count for very large files\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, nrows=0)  # Just get columns\n",
    "            columns = list(df.columns)\n",
    "            \n",
    "            # Count rows efficiently\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                row_count = sum(1 for line in f) - 1  # Subtract header\n",
    "            \n",
    "            size_mb = Path(filepath).stat().st_size / (1024 * 1024)\n",
    "            \n",
    "            dataset_summary.append({\n",
    "                'Dataset': name,\n",
    "                'Status': '‚úÖ',\n",
    "                'Rows': f\"{row_count:,}\",\n",
    "                'Columns': len(columns),\n",
    "                'Size (MB)': f\"{size_mb:.1f}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ {name}\")\n",
    "            print(f\"   Rows: {row_count:,} | Columns: {len(columns)} | Size: {size_mb:.1f} MB\")\n",
    "            print(f\"   Columns: {', '.join(columns[:8])}{'...' if len(columns) > 8 else ''}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            dataset_summary.append({\n",
    "                'Dataset': name,\n",
    "                'Status': '‚ö†Ô∏è ERROR',\n",
    "                'Rows': str(e)[:50],\n",
    "                'Columns': '-',\n",
    "                'Size (MB)': '-'\n",
    "            })\n",
    "            print(f\"‚ö†Ô∏è  {name}: Error - {str(e)[:50]}\")\n",
    "            print()\n",
    "    else:\n",
    "        dataset_summary.append({\n",
    "            'Dataset': name,\n",
    "            'Status': '‚ùå',\n",
    "            'Rows': 'Not found',\n",
    "            'Columns': '-',\n",
    "            'Size (MB)': '-'\n",
    "        })\n",
    "        print(f\"‚ùå {name}: NOT FOUND\")\n",
    "        print()\n",
    "\n",
    "# Display summary table\n",
    "summary_df = pd.DataFrame(dataset_summary)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20069d5b",
   "metadata": {},
   "source": [
    "## 4. üîç Detailed Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36862eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DETAILED COLUMN STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Crisis data\n",
    "if Path('standardized_data/crisis_combined.csv').exists():\n",
    "    print(\"üìä CRISIS COMBINED\")\n",
    "    print(\"-\" * 80)\n",
    "    crisis_df = pd.read_csv('standardized_data/crisis_combined.csv', nrows=3)\n",
    "    print(f\"Columns: {list(crisis_df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{crisis_df.dtypes}\")\n",
    "    print(f\"\\nFirst row sample:\")\n",
    "    display(crisis_df.head(1).T)\n",
    "    print()\n",
    "\n",
    "# Non-crisis data  \n",
    "if Path('standardized_data/non_crisis_combined.csv').exists():\n",
    "    print(\"\\nüìä NON-CRISIS COMBINED\")\n",
    "    print(\"-\" * 80)\n",
    "    non_crisis_df = pd.read_csv('standardized_data/non_crisis_combined.csv', nrows=3)\n",
    "    print(f\"Columns: {list(non_crisis_df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{non_crisis_df.dtypes}\")\n",
    "    print(f\"\\nFirst row sample:\")\n",
    "    display(non_crisis_df.head(1).T)\n",
    "    print()\n",
    "\n",
    "# GoEmotions\n",
    "if Path('goemotion_data/goemotions.csv').exists():\n",
    "    print(\"\\nüìä GOEMOTIONS (27 EMOTIONS)\")\n",
    "    print(\"-\" * 80)\n",
    "    goemo_df = pd.read_csv('goemotion_data/goemotions.csv', nrows=3)\n",
    "    print(f\"Columns: {list(goemo_df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{goemo_df.dtypes}\")\n",
    "    print(f\"\\nFirst row sample:\")\n",
    "    display(goemo_df.head(1).T)\n",
    "else:\n",
    "    print(\"\\n‚ùå GoEmotions NOT FOUND - Need to download from Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385fbdf",
   "metadata": {},
   "source": [
    "## 5. ‚ö†Ô∏è Issues & Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ISSUES & MISSING COMPONENTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check for GoEmotions\n",
    "if not Path('goemotion_data/goemotions.csv').exists():\n",
    "    issues.append(\"‚ùå GoEmotions data missing - CRITICAL for emotion mapping\")\n",
    "\n",
    "# Check for emotion columns in crisis/non-crisis data\n",
    "if Path('standardized_data/crisis_combined.csv').exists():\n",
    "    crisis_df = pd.read_csv('standardized_data/crisis_combined.csv', nrows=1)\n",
    "    emotion_cols = [col for col in crisis_df.columns if 'emotion' in col.lower()]\n",
    "    if not emotion_cols:\n",
    "        issues.append(\"‚ö†Ô∏è  No emotion columns in crisis_combined.csv - needs to be added\")\n",
    "\n",
    "if Path('standardized_data/non_crisis_combined.csv').exists():\n",
    "    non_crisis_df = pd.read_csv('standardized_data/non_crisis_combined.csv', nrows=1)\n",
    "    emotion_cols = [col for col in non_crisis_df.columns if 'emotion' in col.lower()]\n",
    "    if not emotion_cols:\n",
    "        issues.append(\"‚ö†Ô∏è  No emotion columns in non_crisis_combined.csv - needs to be added\")\n",
    "\n",
    "# Check for master training data\n",
    "if not Path('master_training_data/master_training_data.csv').exists():\n",
    "    issues.append(\"‚ö†Ô∏è  Master training file missing - may need to be regenerated\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"‚úÖ No major issues detected!\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"RECOMMENDED ACTIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. Download goemotion_data/ from Google Drive (if missing)\n",
    "2. Define 13 target emotions for crisis detection\n",
    "3. Create 27‚Üí13 emotion mapping using LLM\n",
    "4. Add emotion_label column to standardized datasets\n",
    "5. Refactor code to industry standards\n",
    "6. Convert Python scripts to notebooks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8fee5",
   "metadata": {},
   "source": [
    "## 6. üìà Quick Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QUICK STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "stats = {}\n",
    "\n",
    "# Crisis data stats\n",
    "if Path('standardized_data/crisis_combined.csv').exists():\n",
    "    crisis_df = pd.read_csv('standardized_data/crisis_combined.csv')\n",
    "    print(\"üìä Crisis Data:\")\n",
    "    print(f\"   Total tweets: {len(crisis_df):,}\")\n",
    "    if 'event_type' in crisis_df.columns:\n",
    "        print(f\"   Unique event types: {crisis_df['event_type'].nunique()}\")\n",
    "        print(f\"   Top 5 event types:\")\n",
    "        for event, count in crisis_df['event_type'].value_counts().head(5).items():\n",
    "            print(f\"      {event}: {count:,}\")\n",
    "    print()\n",
    "\n",
    "# Non-crisis data stats\n",
    "if Path('standardized_data/non_crisis_combined.csv').exists():\n",
    "    non_crisis_df = pd.read_csv('standardized_data/non_crisis_combined.csv')\n",
    "    print(\"üìä Non-Crisis Data:\")\n",
    "    print(f\"   Total tweets: {len(non_crisis_df):,}\")\n",
    "    if 'event_name' in non_crisis_df.columns:\n",
    "        print(f\"   Unique events: {non_crisis_df['event_name'].nunique()}\")\n",
    "        print(f\"   Top 5 events:\")\n",
    "        for event, count in non_crisis_df['event_name'].value_counts().head(5).items():\n",
    "            print(f\"      {event}: {count:,}\")\n",
    "    print()\n",
    "\n",
    "# Combined totals\n",
    "if 'crisis_df' in dir() and 'non_crisis_df' in dir():\n",
    "    total_tweets = len(crisis_df) + len(non_crisis_df)\n",
    "    print(f\"üìä TOTAL DATASET: {total_tweets:,} tweets\")\n",
    "    print(f\"   Crisis: {len(crisis_df):,} ({len(crisis_df)/total_tweets*100:.1f}%)\")\n",
    "    print(f\"   Non-crisis: {len(non_crisis_df):,} ({len(non_crisis_df)/total_tweets*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
