{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6b1cb9",
   "metadata": {},
   "source": [
    "# üìä TEMPO Project Status Dashboard\n",
    "## Visual Overview of Data Files and Structure\n",
    "\n",
    "This notebook provides a complete visual summary of:\n",
    "- What data folders exist\n",
    "- File sizes and row counts\n",
    "- Column structures\n",
    "- What's missing vs. what you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd622211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ef0e0",
   "metadata": {},
   "source": [
    "## 1. üìÇ Directory Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37f668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA FOLDER STATUS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ EXISTS  baseline_data                  (   138.9 MB)\n",
      "‚úÖ EXISTS  crisis_datasets                (    51.6 MB)\n",
      "‚úÖ EXISTS  goemotion_data                 (     9.4 MB)\n",
      "‚úÖ EXISTS  master_training_data           (   277.9 MB)\n",
      "‚úÖ EXISTS  non_crisis_data                (  1938.2 MB)\n",
      "‚úÖ EXISTS  standardized_data              (   557.2 MB)\n",
      "\n",
      "Summary: 6/6 folders present\n"
     ]
    }
   ],
   "source": [
    "# Expected data folders\n",
    "expected_folders = [\n",
    "    'baseline_data',\n",
    "    'crisis_datasets', \n",
    "    'goemotion_data',\n",
    "    'master_training_data',\n",
    "    'non_crisis_data',\n",
    "    'standardized_data'\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA FOLDER STATUS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "folder_status = {}\n",
    "for folder in expected_folders:\n",
    "    exists = Path(folder).exists()\n",
    "    status = \"‚úÖ EXISTS\" if exists else \"‚ùå MISSING\"\n",
    "    folder_status[folder] = exists\n",
    "    \n",
    "    # Get size if exists\n",
    "    if exists:\n",
    "        total_size = sum(f.stat().st_size for f in Path(folder).rglob('*') if f.is_file())\n",
    "        size_mb = total_size / (1024 * 1024)\n",
    "        print(f\"{status}  {folder:<30} ({size_mb:>8.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"{status}  {folder:<30} (NOT DOWNLOADED)\")\n",
    "\n",
    "print()\n",
    "print(f\"Summary: {sum(folder_status.values())}/{len(expected_folders)} folders present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6405c7",
   "metadata": {},
   "source": [
    "## 2. üìÑ CSV Files Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b1c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FOUND 115 CSV FILES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìÅ baseline_data/\n",
      "--------------------------------------------------------------------------------\n",
      "   baseline_noise.csv                              138.88 MB\n",
      "\n",
      "üìÅ crisis_datasets/\n",
      "--------------------------------------------------------------------------------\n",
      "   crisislex_all_combined.csv                        5.09 MB\n",
      "   crisislex_all_complete.csv                        6.85 MB\n",
      "   humaid_all_with_timestamps.csv                    9.88 MB\n",
      "\n",
      "üìÅ crisis_datasets/crisislex_complete/\n",
      "--------------------------------------------------------------------------------\n",
      "   2012_Colorado_wildfires_complete.csv              0.35 MB\n",
      "   2012_Costa_Rica_earthquake_complete.csv           0.41 MB\n",
      "   2012_Guatemala_earthquake_complete.csv            0.31 MB\n",
      "   2012_Italy_earthquakes_complete.csv               0.29 MB\n",
      "   2012_Philipinnes_floods_complete.csv              0.30 MB\n",
      "   2012_Typhoon_Pablo_complete.csv                   0.28 MB\n",
      "   2012_Venezuela_refinery_complete.csv              0.30 MB\n",
      "   2013_Alberta_floods_complete.csv                  0.30 MB\n",
      "   2013_Australia_bushfire_complete.csv              0.36 MB\n",
      "   2013_Bohol_earthquake_complete.csv                0.29 MB\n",
      "   2013_Boston_bombings_complete.csv                 0.29 MB\n",
      "   2013_Brazil_nightclub_fire_complete.csv           0.30 MB\n",
      "   2013_Colorado_floods_complete.csv                 0.29 MB\n",
      "   2013_Manila_floods_complete.csv                   0.29 MB\n",
      "   2013_NY_train_crash_complete.csv                  0.30 MB\n",
      "   2013_Queensland_floods_complete.csv               0.34 MB\n",
      "   2013_Sardinia_floods_complete.csv                 0.30 MB\n",
      "   2013_Savar_building_collapse_complete.csv         0.38 MB\n",
      "   2013_Singapore_haze_complete.csv                  0.28 MB\n",
      "   2013_Spain_train_crash_complete.csv               0.30 MB\n",
      "   2013_Typhoon_Yolanda_complete.csv                 0.31 MB\n",
      "   2013_West_Texas_explosion_complete.csv            0.29 MB\n",
      "\n",
      "üìÅ crisis_datasets/crisislex_data/\n",
      "--------------------------------------------------------------------------------\n",
      "   2012_Colorado_wildfires.csv                       0.26 MB\n",
      "   2012_Costa_Rica_earthquake.csv                    0.30 MB\n",
      "   2012_Guatemala_earthquake.csv                     0.23 MB\n",
      "   2012_Italy_earthquakes.csv                        0.22 MB\n",
      "   2012_Philipinnes_floods.csv                       0.22 MB\n",
      "   2012_Typhoon_Pablo.csv                            0.21 MB\n",
      "   2012_Venezuela_refinery.csv                       0.23 MB\n",
      "   2013_Alberta_floods.csv                           0.22 MB\n",
      "   2013_Australia_bushfire.csv                       0.27 MB\n",
      "   2013_Bohol_earthquake.csv                         0.21 MB\n",
      "   2013_Boston_bombings.csv                          0.22 MB\n",
      "   2013_Brazil_nightclub_fire.csv                    0.22 MB\n",
      "   2013_Colorado_floods.csv                          0.22 MB\n",
      "   2013_Manila_floods.csv                            0.21 MB\n",
      "   2013_NY_train_crash.csv                           0.22 MB\n",
      "   2013_Queensland_floods.csv                        0.25 MB\n",
      "   2013_Sardinia_floods.csv                          0.22 MB\n",
      "   2013_Savar_building_collapse.csv                  0.28 MB\n",
      "   2013_Singapore_haze.csv                           0.21 MB\n",
      "   2013_Spain_train_crash.csv                        0.23 MB\n",
      "   2013_Typhoon_Yolanda.csv                          0.23 MB\n",
      "   2013_West_Texas_explosion.csv                     0.22 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/canada_wildfires_2016/\n",
      "--------------------------------------------------------------------------------\n",
      "   canada_wildfires_2016_dev_with_timestamps.csv     0.04 MB\n",
      "   canada_wildfires_2016_test_with_timestamps.csv     0.08 MB\n",
      "   canada_wildfires_2016_train_with_timestamps.csv     0.29 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/cyclone_idai_2019/\n",
      "--------------------------------------------------------------------------------\n",
      "   cyclone_idai_2019_dev_with_timestamps.csv         0.11 MB\n",
      "   cyclone_idai_2019_test_with_timestamps.csv        0.21 MB\n",
      "   cyclone_idai_2019_train_with_timestamps.csv       0.75 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/ecuador_earthquake_2016/\n",
      "--------------------------------------------------------------------------------\n",
      "   ecuador_earthquake_2016_dev_with_timestamps.csv     0.03 MB\n",
      "   ecuador_earthquake_2016_test_with_timestamps.csv     0.06 MB\n",
      "   ecuador_earthquake_2016_train_with_timestamps.csv     0.20 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/hurricane_harvey_2017/\n",
      "--------------------------------------------------------------------------------\n",
      "   hurricane_harvey_2017_dev_with_timestamps.csv     0.17 MB\n",
      "   hurricane_harvey_2017_test_with_timestamps.csv     0.33 MB\n",
      "   hurricane_harvey_2017_train_with_timestamps.csv     1.16 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/hurricane_irma_2017/\n",
      "--------------------------------------------------------------------------------\n",
      "   hurricane_irma_2017_dev_with_timestamps.csv       0.17 MB\n",
      "   hurricane_irma_2017_test_with_timestamps.csv      0.33 MB\n",
      "   hurricane_irma_2017_train_with_timestamps.csv     1.18 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/hurricane_maria_2017/\n",
      "--------------------------------------------------------------------------------\n",
      "   hurricane_maria_2017_dev_with_timestamps.csv      0.14 MB\n",
      "   hurricane_maria_2017_test_with_timestamps.csv     0.27 MB\n",
      "   hurricane_maria_2017_train_with_timestamps.csv     0.97 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/hurricane_matthew_2016/\n",
      "--------------------------------------------------------------------------------\n",
      "   hurricane_matthew_2016_dev_with_timestamps.csv     0.03 MB\n",
      "   hurricane_matthew_2016_test_with_timestamps.csv     0.06 MB\n",
      "   hurricane_matthew_2016_train_with_timestamps.csv     0.21 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/hurricane_matthew_2016/greece_wildfires_2018/\n",
      "--------------------------------------------------------------------------------\n",
      "   greece_wildfires_2018_dev_with_timestamps.csv     0.04 MB\n",
      "   greece_wildfires_2018_test_with_timestamps.csv     0.07 MB\n",
      "   greece_wildfires_2018_train_with_timestamps.csv     0.24 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/hurricane_matthew_2016/maryland_floods_2018/\n",
      "--------------------------------------------------------------------------------\n",
      "   maryland_floods_2018_dev_with_timestamps.csv      0.02 MB\n",
      "   maryland_floods_2018_test_with_timestamps.csv     0.03 MB\n",
      "   maryland_floods_2018_train_with_timestamps.csv     0.12 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/italy_earthquake_aug_2016/\n",
      "--------------------------------------------------------------------------------\n",
      "   italy_earthquake_aug_2016_dev_with_timestamps.csv     0.02 MB\n",
      "   italy_earthquake_aug_2016_test_with_timestamps.csv     0.04 MB\n",
      "   italy_earthquake_aug_2016_train_with_timestamps.csv     0.15 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/kaikoura_earthquake_2016/\n",
      "--------------------------------------------------------------------------------\n",
      "   kaikoura_earthquake_2016_dev_with_timestamps.csv     0.04 MB\n",
      "   kaikoura_earthquake_2016_test_with_timestamps.csv     0.08 MB\n",
      "   kaikoura_earthquake_2016_train_with_timestamps.csv     0.27 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/puebla_mexico_earthquake_2017/\n",
      "--------------------------------------------------------------------------------\n",
      "   puebla_mexico_earthquake_2017_dev_with_timestamps.csv     0.04 MB\n",
      "   puebla_mexico_earthquake_2017_test_with_timestamps.csv     0.08 MB\n",
      "   puebla_mexico_earthquake_2017_train_with_timestamps.csv     0.27 MB\n",
      "\n",
      "üìÅ crisis_datasets/humaid_crisis_data/events_set1/srilanka_floods_2017/\n",
      "--------------------------------------------------------------------------------\n",
      "   srilanka_floods_2017_dev_with_timestamps.csv      0.01 MB\n",
      "   srilanka_floods_2017_test_with_timestamps.csv     0.02 MB\n",
      "   srilanka_floods_2017_train_with_timestamps.csv     0.08 MB\n",
      "\n",
      "üìÅ goemotion_data/\n",
      "--------------------------------------------------------------------------------\n",
      "   goemotions.csv                                    4.50 MB\n",
      "   goemotions_with_13_emotions.csv                   4.89 MB\n",
      "\n",
      "üìÅ master_training_data/\n",
      "--------------------------------------------------------------------------------\n",
      "   master_training_data.csv                        277.78 MB\n",
      "   master_training_sample_1000.csv                   0.16 MB\n",
      "\n",
      "üìÅ non_crisis_data/\n",
      "--------------------------------------------------------------------------------\n",
      "   non_crisis_summary.csv                            0.00 MB\n",
      "\n",
      "üìÅ non_crisis_data/kaggle_downloads/worldcup_2018/\n",
      "--------------------------------------------------------------------------------\n",
      "   FIFA.csv                                        175.13 MB\n",
      "\n",
      "üìÅ non_crisis_data/music_artists/\n",
      "--------------------------------------------------------------------------------\n",
      "   tweets-from-music-artists_balanced-dataset.csv     0.55 MB\n",
      "\n",
      "üìÅ non_crisis_data/non_crisis_data/\n",
      "--------------------------------------------------------------------------------\n",
      "   coachella.csv                                     0.65 MB\n",
      "   fifa_worldcup_2022.csv                           26.69 MB\n",
      "   game_of_thrones.csv                             681.35 MB\n",
      "   music_artists.csv                                 0.55 MB\n",
      "   tokyo_olympics_2020.csv                          60.83 MB\n",
      "   tokyo_olympics_2020.meow.csv                     59.87 MB\n",
      "   us_election_2020.csv                             48.80 MB\n",
      "\n",
      "üìÅ non_crisis_data/tokyo_olympics_2020/\n",
      "--------------------------------------------------------------------------------\n",
      "   olympics_sample.csv                              28.37 MB\n",
      "\n",
      "üìÅ non_crisis_data/us_election_2020/\n",
      "--------------------------------------------------------------------------------\n",
      "   hashtag_donaldtrump.csv                         410.23 MB\n",
      "   hashtag_joebiden.csv                            363.18 MB\n",
      "\n",
      "üìÅ standardized_data/\n",
      "--------------------------------------------------------------------------------\n",
      "   coachella_standardized.csv                        0.56 MB\n",
      "   crisis_combined.csv                              12.91 MB\n",
      "   crisislex_standardized.csv                        4.59 MB\n",
      "   fifa_worldcup_standardized.csv                   12.00 MB\n",
      "   game_of_thrones_standardized.csv                142.61 MB\n",
      "   humaid_standardized.csv                           8.32 MB\n",
      "   music_concerts_standardized.csv                   0.45 MB\n",
      "   non_crisis_combined.csv                         265.68 MB\n",
      "   tokyo_olympics_standardized.csv                  29.49 MB\n",
      "   us_election_standardized.csv                     23.67 MB\n",
      "   worldcup_2018_standardized.csv                   56.89 MB\n"
     ]
    }
   ],
   "source": [
    "# Find all CSV files\n",
    "csv_files = list(Path('.').rglob('*.csv'))\n",
    "\n",
    "# Filter out git and venv folders\n",
    "csv_files = [f for f in csv_files if '.git' not in str(f) and 'venv' not in str(f) and '.venv' not in str(f)]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"FOUND {len(csv_files)} CSV FILES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Group by folder\n",
    "from collections import defaultdict\n",
    "files_by_folder = defaultdict(list)\n",
    "\n",
    "for f in csv_files:\n",
    "    folder = str(f.parent)\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    files_by_folder[folder].append({\n",
    "        'name': f.name,\n",
    "        'size_mb': size_mb,\n",
    "        'path': str(f)\n",
    "    })\n",
    "\n",
    "# Display by folder\n",
    "for folder in sorted(files_by_folder.keys()):\n",
    "    print(f\"\\nüìÅ {folder}/\")\n",
    "    print(\"-\" * 80)\n",
    "    for file_info in sorted(files_by_folder[folder], key=lambda x: x['name']):\n",
    "        print(f\"   {file_info['name']:<45} {file_info['size_mb']:>8.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a6b63",
   "metadata": {},
   "source": [
    "## 3. üìä Dataset Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1527215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KEY DATASETS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Crisis Combined\n",
      "   Rows: 66,766 | Columns: 7 | Size: 12.9 MB\n",
      "   Columns: text, created_at, event_name, event_type, crisis_label, source_dataset, informativeness\n",
      "\n",
      "‚úÖ Non-Crisis Combined\n",
      "   Rows: 2,306,740 | Columns: 6 | Size: 265.7 MB\n",
      "   Columns: text, created_at, event_name, event_type, crisis_label, source_dataset\n",
      "\n",
      "‚úÖ HumAID Standardized\n",
      "   Rows: 43,409 | Columns: 7 | Size: 8.3 MB\n",
      "   Columns: text, created_at, event_name, event_type, crisis_label, source_dataset, informativeness\n",
      "\n",
      "‚úÖ CrisisLex Standardized\n",
      "   Rows: 23,357 | Columns: 7 | Size: 4.6 MB\n",
      "   Columns: text, created_at, event_name, event_type, crisis_label, source_dataset, informativeness\n",
      "\n",
      "‚úÖ GoEmotions (27 emotions)\n",
      "   Rows: 54,263 | Columns: 5 | Size: 4.5 MB\n",
      "   Columns: text, labels, id, Unnamed: 3, [27] = neutral [0] = admiration [1] = amusement [2] = anger [3] = annoyance [4] = approval [5] = caring [6] = confusion [7] = curiosity [8] = desire [9] = disappointment [10] = disapproval [11] = disgust [12] = embarrassment [13] = excitement [14] = fear [15] = gratitude [16] = grief [17] = joy [18] = love [19] = nervousness [20] = optimism [21] = pride [22] = realization [23] = relief [24] = remorse [25] = sadness [26] = surprise [27] = neutral\n",
      "\n",
      "‚úÖ Master Training File\n",
      "   Rows: 2,427,769 | Columns: 19 | Size: 277.8 MB\n",
      "   Columns: text, emotion_fear, emotion_sadness, emotion_anger, emotion_nervousness, emotion_disgust, emotion_surprise, emotion_confusion...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Status</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Size (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crisis Combined</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>66,766</td>\n",
       "      <td>7</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Crisis Combined</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>2,306,740</td>\n",
       "      <td>6</td>\n",
       "      <td>265.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HumAID Standardized</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>43,409</td>\n",
       "      <td>7</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrisisLex Standardized</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>23,357</td>\n",
       "      <td>7</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GoEmotions (27 emotions)</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>54,263</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Master Training File</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>2,427,769</td>\n",
       "      <td>19</td>\n",
       "      <td>277.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Dataset Status       Rows  Columns Size (MB)\n",
       "0           Crisis Combined      ‚úÖ     66,766        7      12.9\n",
       "1       Non-Crisis Combined      ‚úÖ  2,306,740        6     265.7\n",
       "2       HumAID Standardized      ‚úÖ     43,409        7       8.3\n",
       "3    CrisisLex Standardized      ‚úÖ     23,357        7       4.6\n",
       "4  GoEmotions (27 emotions)      ‚úÖ     54,263        5       4.5\n",
       "5      Master Training File      ‚úÖ  2,427,769       19     277.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Key datasets to analyze\n",
    "key_datasets = [\n",
    "    ('standardized_data/crisis_combined.csv', 'Crisis Combined'),\n",
    "    ('standardized_data/non_crisis_combined.csv', 'Non-Crisis Combined'),\n",
    "    ('standardized_data/humaid_standardized.csv', 'HumAID Standardized'),\n",
    "    ('standardized_data/crisislex_standardized.csv', 'CrisisLex Standardized'),\n",
    "    ('goemotion_data/goemotions.csv', 'GoEmotions (27 emotions)'),\n",
    "    ('master_training_data/master_training_data.csv', 'Master Training File'),\n",
    "]\n",
    "\n",
    "dataset_summary = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY DATASETS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for filepath, name in key_datasets:\n",
    "    if Path(filepath).exists():\n",
    "        # Quick check using line count for very large files\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, nrows=0)  # Just get columns\n",
    "            columns = list(df.columns)\n",
    "            \n",
    "            # Count rows efficiently\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                row_count = sum(1 for line in f) - 1  # Subtract header\n",
    "            \n",
    "            size_mb = Path(filepath).stat().st_size / (1024 * 1024)\n",
    "            \n",
    "            dataset_summary.append({\n",
    "                'Dataset': name,\n",
    "                'Status': '‚úÖ',\n",
    "                'Rows': f\"{row_count:,}\",\n",
    "                'Columns': len(columns),\n",
    "                'Size (MB)': f\"{size_mb:.1f}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ {name}\")\n",
    "            print(f\"   Rows: {row_count:,} | Columns: {len(columns)} | Size: {size_mb:.1f} MB\")\n",
    "            print(f\"   Columns: {', '.join(columns[:8])}{'...' if len(columns) > 8 else ''}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            dataset_summary.append({\n",
    "                'Dataset': name,\n",
    "                'Status': '‚ö†Ô∏è ERROR',\n",
    "                'Rows': str(e)[:50],\n",
    "                'Columns': '-',\n",
    "                'Size (MB)': '-'\n",
    "            })\n",
    "            print(f\"‚ö†Ô∏è  {name}: Error - {str(e)[:50]}\")\n",
    "            print()\n",
    "    else:\n",
    "        dataset_summary.append({\n",
    "            'Dataset': name,\n",
    "            'Status': '‚ùå',\n",
    "            'Rows': 'Not found',\n",
    "            'Columns': '-',\n",
    "            'Size (MB)': '-'\n",
    "        })\n",
    "        print(f\"‚ùå {name}: NOT FOUND\")\n",
    "        print()\n",
    "\n",
    "# Display summary table\n",
    "summary_df = pd.DataFrame(dataset_summary)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20069d5b",
   "metadata": {},
   "source": [
    "## 4. üîç Detailed Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36862eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAILED COLUMN STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "üìä CRISIS COMBINED\n",
      "--------------------------------------------------------------------------------\n",
      "Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset', 'informativeness']\n",
      "\n",
      "Data types:\n",
      "text                   str\n",
      "created_at             str\n",
      "event_name             str\n",
      "event_type             str\n",
      "crisis_label         int64\n",
      "source_dataset         str\n",
      "informativeness    float64\n",
      "dtype: object\n",
      "\n",
      "First row sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>.@GreenABEnergy How can @AirworksCanada assist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>2016-05-19 18:16:11.727000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_name</th>\n",
       "      <td>canada_wildfires_2016_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <td>wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crisis_label</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_dataset</th>\n",
       "      <td>humaid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informativeness</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0\n",
       "text             .@GreenABEnergy How can @AirworksCanada assist...\n",
       "created_at                        2016-05-19 18:16:11.727000+00:00\n",
       "event_name                               canada_wildfires_2016_dev\n",
       "event_type                                                wildfire\n",
       "crisis_label                                                     1\n",
       "source_dataset                                              humaid\n",
       "informativeness                                                NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìä NON-CRISIS COMBINED\n",
      "--------------------------------------------------------------------------------\n",
      "Columns: ['text', 'created_at', 'event_name', 'event_type', 'crisis_label', 'source_dataset']\n",
      "\n",
      "Data types:\n",
      "text                str\n",
      "created_at          str\n",
      "event_name          str\n",
      "event_type          str\n",
      "crisis_label      int64\n",
      "source_dataset      str\n",
      "dtype: object\n",
      "\n",
      "First row sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>#Coachella2015 tickets selling out in less tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>2015-01-07 15:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_name</th>\n",
       "      <td>coachella_2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crisis_label</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_dataset</th>\n",
       "      <td>coachella</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                0\n",
       "text            #Coachella2015 tickets selling out in less tha...\n",
       "created_at                                    2015-01-07 15:02:00\n",
       "event_name                                         coachella_2015\n",
       "event_type                                          entertainment\n",
       "crisis_label                                                    0\n",
       "source_dataset                                          coachella"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìä GOEMOTIONS (27 EMOTIONS)\n",
      "--------------------------------------------------------------------------------\n",
      "Columns: ['text', 'labels', 'id', 'Unnamed: 3', '[27] = neutral [0] = admiration [1] = amusement [2] = anger [3] = annoyance [4] = approval [5] = caring [6] = confusion [7] = curiosity [8] = desire [9] = disappointment [10] = disapproval [11] = disgust [12] = embarrassment [13] = excitement [14] = fear [15] = gratitude [16] = grief [17] = joy [18] = love [19] = nervousness [20] = optimism [21] = pride [22] = realization [23] = relief [24] = remorse [25] = sadness [26] = surprise [27] = neutral']\n",
      "\n",
      "Data types:\n",
      "text                                                                                                                                                                                                                                                                                                                                                                                                                                                                     str\n",
      "labels                                                                                                                                                                                                                                                                                                                                                                                                                                                                   str\n",
      "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                       str\n",
      "Unnamed: 3                                                                                                                                                                                                                                                                                                                                                                                                                                                           float64\n",
      "[27] = neutral [0] = admiration [1] = amusement [2] = anger [3] = annoyance [4] = approval [5] = caring [6] = confusion [7] = curiosity [8] = desire [9] = disappointment [10] = disapproval [11] = disgust [12] = embarrassment [13] = excitement [14] = fear [15] = gratitude [16] = grief [17] = joy [18] = love [19] = nervousness [20] = optimism [21] = pride [22] = realization [23] = relief [24] = remorse [25] = sadness [26] = surprise [27] = neutral    float64\n",
      "dtype: object\n",
      "\n",
      "First row sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>[27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[27] = neutral [0] = admiration [1] = amusement [2] = anger [3] = annoyance [4] = approval [5] = caring [6] = confusion [7] = curiosity [8] = desire [9] = disappointment [10] = disapproval [11] = disgust [12] = embarrassment [13] = excitement [14] = fear [15] = gratitude [16] = grief [17] = joy [18] = love [19] = nervousness [20] = optimism [21] = pride [22] = realization [23] = relief [24] = remorse [25] = sadness [26] = surprise [27] = neutral</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    0\n",
       "text                                                My favourite food is anything I didn't have to...\n",
       "labels                                                                                           [27]\n",
       "id                                                                                            eebbqej\n",
       "Unnamed: 3                                                                                        NaN\n",
       "[27] = neutral [0] = admiration [1] = amusement...                                                NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DETAILED COLUMN STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Crisis data\n",
    "if Path('standardized_data/crisis_combined.csv').exists():\n",
    "    print(\"üìä CRISIS COMBINED\")\n",
    "    print(\"-\" * 80)\n",
    "    crisis_df = pd.read_csv('standardized_data/crisis_combined.csv', nrows=3)\n",
    "    print(f\"Columns: {list(crisis_df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{crisis_df.dtypes}\")\n",
    "    print(f\"\\nFirst row sample:\")\n",
    "    display(crisis_df.head(1).T)\n",
    "    print()\n",
    "\n",
    "# Non-crisis data  \n",
    "if Path('standardized_data/non_crisis_combined.csv').exists():\n",
    "    print(\"\\nüìä NON-CRISIS COMBINED\")\n",
    "    print(\"-\" * 80)\n",
    "    non_crisis_df = pd.read_csv('standardized_data/non_crisis_combined.csv', nrows=3)\n",
    "    print(f\"Columns: {list(non_crisis_df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{non_crisis_df.dtypes}\")\n",
    "    print(f\"\\nFirst row sample:\")\n",
    "    display(non_crisis_df.head(1).T)\n",
    "    print()\n",
    "\n",
    "# GoEmotions\n",
    "if Path('goemotion_data/goemotions.csv').exists():\n",
    "    print(\"\\nüìä GOEMOTIONS (27 EMOTIONS)\")\n",
    "    print(\"-\" * 80)\n",
    "    goemo_df = pd.read_csv('goemotion_data/goemotions.csv', nrows=3)\n",
    "    print(f\"Columns: {list(goemo_df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{goemo_df.dtypes}\")\n",
    "    print(f\"\\nFirst row sample:\")\n",
    "    display(goemo_df.head(1).T)\n",
    "else:\n",
    "    print(\"\\n‚ùå GoEmotions NOT FOUND - Need to download from Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385fbdf",
   "metadata": {},
   "source": [
    "## 5. ‚ö†Ô∏è Issues & Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a6b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ISSUES & MISSING COMPONENTS\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  No emotion columns in crisis_combined.csv - needs to be added\n",
      "‚ö†Ô∏è  No emotion columns in non_crisis_combined.csv - needs to be added\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED ACTIONS\n",
      "================================================================================\n",
      "\n",
      "1. Download goemotion_data/ from Google Drive (if missing)\n",
      "2. Define 13 target emotions for crisis detection\n",
      "3. Create 27‚Üí13 emotion mapping using LLM\n",
      "4. Add emotion_label column to standardized datasets\n",
      "5. Refactor code to industry standards\n",
      "6. Convert Python scripts to notebooks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ISSUES & MISSING COMPONENTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check for GoEmotions\n",
    "if not Path('goemotion_data/goemotions.csv').exists():\n",
    "    issues.append(\"‚ùå GoEmotions data missing - CRITICAL for emotion mapping\")\n",
    "\n",
    "# Check for emotion columns in crisis/non-crisis data\n",
    "if Path('standardized_data/crisis_combined.csv').exists():\n",
    "    crisis_df = pd.read_csv('standardized_data/crisis_combined.csv', nrows=1)\n",
    "    emotion_cols = [col for col in crisis_df.columns if 'emotion' in col.lower()]\n",
    "    if not emotion_cols:\n",
    "        issues.append(\"‚ö†Ô∏è  No emotion columns in crisis_combined.csv - needs to be added\")\n",
    "\n",
    "if Path('standardized_data/non_crisis_combined.csv').exists():\n",
    "    non_crisis_df = pd.read_csv('standardized_data/non_crisis_combined.csv', nrows=1)\n",
    "    emotion_cols = [col for col in non_crisis_df.columns if 'emotion' in col.lower()]\n",
    "    if not emotion_cols:\n",
    "        issues.append(\"‚ö†Ô∏è  No emotion columns in non_crisis_combined.csv - needs to be added\")\n",
    "\n",
    "# Check for master training data\n",
    "if not Path('master_training_data/master_training_data.csv').exists():\n",
    "    issues.append(\"‚ö†Ô∏è  Master training file missing - may need to be regenerated\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"‚úÖ No major issues detected!\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"RECOMMENDED ACTIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. Download goemotion_data/ from Google Drive (if missing)\n",
    "2. Define 13 target emotions for crisis detection\n",
    "3. Create 27‚Üí13 emotion mapping using LLM\n",
    "4. Add emotion_label column to standardized datasets\n",
    "5. Refactor code to industry standards\n",
    "6. Convert Python scripts to notebooks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8fee5",
   "metadata": {},
   "source": [
    "## 6. üìà Quick Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e14ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUICK STATISTICS\n",
      "================================================================================\n",
      "\n",
      "üìä Crisis Data:\n",
      "   Total tweets: 66,748\n",
      "   Unique event types: 7\n",
      "   Top 5 event types:\n",
      "      hurricane: 33,422\n",
      "      earthquake: 11,429\n",
      "      flood: 7,498\n",
      "      wildfire: 7,151\n",
      "      accident: 4,248\n",
      "\n",
      "üìä Non-Crisis Data:\n",
      "   Total tweets: 1,533,696\n",
      "   Unique events: 7\n",
      "   Top 5 events:\n",
      "      got_season8_2019: 760,614\n",
      "      fifa_worldcup_2018: 458,533\n",
      "      tokyo_olympics_2020: 159,432\n",
      "      us_election_2020: 99,948\n",
      "      fifa_worldcup_2022: 49,493\n",
      "\n",
      "üìä TOTAL DATASET: 1,600,444 tweets\n",
      "   Crisis: 66,748 (4.2%)\n",
      "   Non-crisis: 1,533,696 (95.8%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QUICK STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "stats = {}\n",
    "\n",
    "# Crisis data stats\n",
    "if Path('standardized_data/crisis_combined.csv').exists():\n",
    "    crisis_df = pd.read_csv('standardized_data/crisis_combined.csv')\n",
    "    print(\"üìä Crisis Data:\")\n",
    "    print(f\"   Total tweets: {len(crisis_df):,}\")\n",
    "    if 'event_type' in crisis_df.columns:\n",
    "        print(f\"   Unique event types: {crisis_df['event_type'].nunique()}\")\n",
    "        print(f\"   Top 5 event types:\")\n",
    "        for event, count in crisis_df['event_type'].value_counts().head(5).items():\n",
    "            print(f\"      {event}: {count:,}\")\n",
    "    print()\n",
    "\n",
    "# Non-crisis data stats\n",
    "if Path('standardized_data/non_crisis_combined.csv').exists():\n",
    "    non_crisis_df = pd.read_csv('standardized_data/non_crisis_combined.csv')\n",
    "    print(\"üìä Non-Crisis Data:\")\n",
    "    print(f\"   Total tweets: {len(non_crisis_df):,}\")\n",
    "    if 'event_name' in non_crisis_df.columns:\n",
    "        print(f\"   Unique events: {non_crisis_df['event_name'].nunique()}\")\n",
    "        print(f\"   Top 5 events:\")\n",
    "        for event, count in non_crisis_df['event_name'].value_counts().head(5).items():\n",
    "            print(f\"      {event}: {count:,}\")\n",
    "    print()\n",
    "\n",
    "# Combined totals\n",
    "if 'crisis_df' in dir() and 'non_crisis_df' in dir():\n",
    "    total_tweets = len(crisis_df) + len(non_crisis_df)\n",
    "    print(f\"üìä TOTAL DATASET: {total_tweets:,} tweets\")\n",
    "    print(f\"   Crisis: {len(crisis_df):,} ({len(crisis_df)/total_tweets*100:.1f}%)\")\n",
    "    print(f\"   Non-crisis: {len(non_crisis_df):,} ({len(non_crisis_df)/total_tweets*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
